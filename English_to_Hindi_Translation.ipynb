{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "haIRQCrEBUlV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "npVsv-MIB1eU",
    "outputId": "e34a1f8d-047b-4300-944a-9d6ad0c075bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yU6ePs2VsmLR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TwXliPqCr6o"
   },
   "outputs": [],
   "source": [
    "#data_cleaning \n",
    "#data_path='/content/drive/My Drive/Colab Notebooks/LanguageTranslation/hin.txt'\n",
    "data_path=r\"E:\\DATASETS\\LanguageTranslation\\hindi_english_data\\hin.txt\"\n",
    "lines= pd.read_table(data_path, names=['eng', 'hin'],encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTpTXrEEjDPu"
   },
   "outputs": [],
   "source": [
    "lines.drop('hin',inplace=True,axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AxRZ2NjlN50"
   },
   "outputs": [],
   "source": [
    "lines.rename(columns={'eng':'hin'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FG6CsCulk-S"
   },
   "outputs": [],
   "source": [
    "\n",
    "lines['eng']=lines.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kr5ZekQjl8dW"
   },
   "outputs": [],
   "source": [
    "lines.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "qVGmelJFmGOC",
    "outputId": "93fe1f8d-d3d2-48e2-e60c-396151c4489a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hin</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>वाह!</td>\n",
       "      <td>Wow!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>बचाओ!</td>\n",
       "      <td>Help!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>उछलो.</td>\n",
       "      <td>Jump.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>कूदो.</td>\n",
       "      <td>Jump.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>छलांग.</td>\n",
       "      <td>Jump.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hin    eng\n",
       "0    वाह!   Wow!\n",
       "1   बचाओ!  Help!\n",
       "2   उछलो.  Jump.\n",
       "3   कूदो.  Jump.\n",
       "4  छलांग.  Jump."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.drop('index',inplace=True,axis='columns')\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jq8XNqk9i5jW"
   },
   "outputs": [],
   "source": [
    "#preprocessing of data\n",
    "# Lowercase all characters\n",
    "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
    "lines.hin=lines.hin.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines.hin=lines.hin.apply(lambda x: re.sub(\"'\", '', x))\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "\n",
    "# Remove all the special characters\n",
    "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.hin=lines.hin.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Remove all numbers from text\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
    "lines.hin = lines.hin.apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
    "lines.hin=lines.hin.apply(lambda x: x.strip())\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.hin=lines.hin.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "lines.hin = lines.hin.apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "-JmRL8iSnlwJ",
    "outputId": "9b284e1a-184e-4377-ad8b-ecc60e00df3a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hin</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>START_ वाह _END</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>START_ बचाओ _END</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>START_ उछलो _END</td>\n",
       "      <td>jump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>START_ कूदो _END</td>\n",
       "      <td>jump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>START_ छलांग _END</td>\n",
       "      <td>jump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>START_ नमस्ते। _END</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>START_ नमस्कार। _END</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>START_ वाहवाह _END</td>\n",
       "      <td>cheers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>START_ चियर्स _END</td>\n",
       "      <td>cheers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>START_ समझे कि नहीं _END</td>\n",
       "      <td>got it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hin     eng\n",
       "0           START_ वाह _END     wow\n",
       "1          START_ बचाओ _END    help\n",
       "2          START_ उछलो _END    jump\n",
       "3          START_ कूदो _END    jump\n",
       "4         START_ छलांग _END    jump\n",
       "5       START_ नमस्ते। _END   hello\n",
       "6      START_ नमस्कार। _END   hello\n",
       "7        START_ वाहवाह _END  cheers\n",
       "8        START_ चियर्स _END  cheers\n",
       "9  START_ समझे कि नहीं _END  got it"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7JrcAPunDy2H"
   },
   "outputs": [],
   "source": [
    "#Vocabulary of English\n",
    "all_eng_words=set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "# Vocabulary of Hindi \n",
    "all_hindi_words=set()\n",
    "for hin in lines.hin:\n",
    "    for word in hin.split():\n",
    "        if word not in all_hindi_words:\n",
    "            all_hindi_words.add(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "LEKFtoMdD0Nh",
    "outputId": "1f714a38-0104-454f-91cf-607dad7ee629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plus\n",
      "jobs\n",
      "hardly\n",
      "brothers\n",
      "team\n",
      "too\n",
      "bombay\n",
      "bags\n",
      "whom\n",
      "kid\n"
     ]
    }
   ],
   "source": [
    "count=-1\n",
    "for i in all_eng_words:\n",
    "    count+=1\n",
    "    if count < 10:\n",
    "        print(i)#printing some words from english words set in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "Fb7g4YeGGo5x",
    "outputId": "60c1bb7b-bd03-45f9-fd87-d8d5390eb533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "उद्योग\n",
      "पकड़ीं\n",
      "जाएँगे।\n",
      "कर\n",
      "चोरी\n",
      "चहिए\n",
      "तोला।\n",
      "गईं\n",
      "द्वार\n",
      "चुना\n"
     ]
    }
   ],
   "source": [
    "count=-1\n",
    "for i in all_hindi_words:\n",
    "    count+=1\n",
    "    if count < 10:\n",
    "        print(i)#printing some words from hindi words set in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9OzbmRbeGrH-",
    "outputId": "4e83919c-b190-4a14-cbbc-6bec1b1f2a78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Length of source sequence or we can english sentence instead of source sequence\n",
    "lenght_list=[]\n",
    "for l in lines.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "max_length_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WYawoDwBHLs8",
    "outputId": "008144ee-7b03-4076-aa57-8d3eef531189"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Length of target sequence or we can hindi sentence instead of source sequence\n",
    "lenght_list=[]\n",
    "for l in lines.hin:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A1QioLlSHQPt",
    "outputId": "31563696-5ad7-424c-c6eb-3866b813db08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder token 2341 decoder token 2968\n"
     ]
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_hindi_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_hindi_words)\n",
    "#max_encoder_seq_length = max([len(txt.split()) for txt in lines.eng])\n",
    "#max_decoder_seq_length = max([len(txt.split()) for txt in lines.hin])\n",
    "print('encoder token',num_encoder_tokens, 'decoder token',num_decoder_tokens)\n",
    "#print('max_encoder_seq_len',max_encoder_seq_length,'max_decoder_seq_len',max_decoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wmQt1o_yI87F",
    "outputId": "8ccf6ed7-e22e-400b-c713-997a8c2b1264"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2969"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3F2tB1310vXV"
   },
   "outputs": [],
   "source": [
    "num_encoder_tokens+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtLOVSROJexE"
   },
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "9ucHaAlx0nP6",
    "outputId": "668ab631-c60d-4062-a029-2979b13463c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'abandoned': 2, 'ability': 3, 'ablaze': 4, 'able': 5, 'about': 6, 'above': 7, 'abroad': 8, 'absence': 9, 'absent': 10, 'absolute': 11, 'absurd': 12, 'abused': 13, 'accepted': 14, 'access': 15, 'accident': 16, 'accidental': 17, 'accompanied': 18, 'accompany': 19, 'according': 20, 'account': 21, 'accountable': 22, 'accused': 23, 'accustomed': 24, 'ache': 25, 'acknowledgement': 26, 'acquaintance': 27, 'acquaintances': 28, 'acquainted': 29, 'across': 30, 'act': 31, 'actions': 32, 'actor': 33, 'actress': 34, 'add': 35, 'adding': 36, 'address': 37, 'admit': 38, 'adopted': 39, 'advantage': 40, 'advice': 41, 'advise': 42, 'advised': 43, 'affected': 44, 'afford': 45, 'afraid': 46, 'africa': 47, 'after': 48, 'afternoon': 49, 'again': 50, 'against': 51, 'age': 52, 'ago': 53, 'agree': 54, 'agreement': 55, 'aids': 56, 'air': 57, 'airport': 58, 'alarm': 59, 'alcohol': 60, 'alike': 61, 'alive': 62, 'all': 63, 'allergic': 64, 'allow': 65, 'allowances': 66, 'allowed': 67, 'almost': 68, 'alone': 69, 'along': 70, 'aloud': 71, 'alphabet': 72, 'already': 73, 'also': 74, 'although': 75, 'always': 76, 'am': 77, 'amateur': 78, 'amazed': 79, 'ambitions': 80, 'ambulance': 81, 'amend': 82, 'america': 83, 'american': 84, 'among': 85, 'amount': 86, 'an': 87, 'anchorage': 88, 'and': 89, 'angels': 90, 'angles': 91, 'angry': 92, 'animal': 93, 'animals': 94, 'annoys': 95, 'another': 96, 'answer': 97, 'answered': 98, 'answering': 99, 'answers': 100, 'anxious': 101, 'any': 102, 'anybody': 103, 'anymore': 104, 'anyone': 105, 'anything': 106, 'anywhere': 107, 'apart': 108, 'apartment': 109, 'apology': 110, 'appearance': 111, 'appeared': 112, 'appearing': 113, 'appears': 114, 'apple': 115, 'apples': 116, 'applies': 117, 'appointment': 118, 'approached': 119, 'april': 120, 'arabic': 121, 'are': 122, 'arent': 123, 'argued': 124, 'arguing': 125, 'arms': 126, 'around': 127, 'arrange': 128, 'arrest': 129, 'arrested': 130, 'arrive': 131, 'arrived': 132, 'arrogance': 133, 'arrogant': 134, 'art': 135, 'artist': 136, 'as': 137, 'ashamed': 138, 'asia': 139, 'ask': 140, 'asked': 141, 'asking': 142, 'asleep': 143, 'assaulted': 144, 'at': 145, 'atheist': 146, 'atmosphere': 147, 'attend': 148, 'attended': 149, 'attention': 150, 'august': 151, 'aunt': 152, 'australia': 153, 'author': 154, 'authorities': 155, 'awake': 156, 'aware': 157, 'away': 158, 'awesome': 159, 'b': 160, 'baby': 161, 'back': 162, 'backs': 163, 'bad': 164, 'bag': 165, 'baggage': 166, 'bags': 167, 'bakery': 168, 'balance': 169, 'ball': 170, 'balls': 171, 'bank': 172, 'bankruptcy': 173, 'bar': 174, 'barbers': 175, 'barely': 176, 'barking': 177, 'baseball': 178, 'basketball': 179, 'bat': 180, 'bath': 181, 'bathroom': 182, 'batter': 183, 'be': 184, 'beach': 185, 'bear': 186, 'beard': 187, 'beast': 188, 'beating': 189, 'beatles': 190, 'beautiful': 191, 'beauty': 192, 'became': 193, 'because': 194, 'become': 195, 'becoming': 196, 'bed': 197, 'been': 198, 'beer': 199, 'before': 200, 'beforehand': 201, 'beg': 202, 'began': 203, 'begin': 204, 'beginning': 205, 'beginnings': 206, 'begins': 207, 'begun': 208, 'behavior': 209, 'behind': 210, 'being': 211, 'believe': 212, 'believes': 213, 'bell': 214, 'bent': 215, 'bern': 216, 'besides': 217, 'best': 218, 'bet': 219, 'betrayed': 220, 'better': 221, 'between': 222, 'bicycle': 223, 'big': 224, 'bigger': 225, 'biggest': 226, 'bill': 227, 'billion': 228, 'biology': 229, 'bird': 230, 'birds': 231, 'birthday': 232, 'bit': 233, 'bite': 234, 'bitter': 235, 'biwa': 236, 'black': 237, 'blame': 238, 'blanket': 239, 'bloody': 240, 'bloom': 241, 'blowing': 242, 'blue': 243, 'board': 244, 'boat': 245, 'bombay': 246, 'bone': 247, 'bones': 248, 'book': 249, 'books': 250, 'booted': 251, 'bored': 252, 'born': 253, 'borrow': 254, 'boss': 255, 'boston': 256, 'both': 257, 'bother': 258, 'bottles': 259, 'bought': 260, 'bounces': 261, 'bound': 262, 'bowls': 263, 'box': 264, 'boy': 265, 'boys': 266, 'branches': 267, 'brave': 268, 'bread': 269, 'break': 270, 'breakfast': 271, 'breath': 272, 'breathe': 273, 'breathed': 274, 'bride': 275, 'bridge': 276, 'bring': 277, 'britain': 278, 'british': 279, 'broke': 280, 'broken': 281, 'brother': 282, 'brothers': 283, 'brought': 284, 'brown': 285, 'brush': 286, 'buddhism': 287, 'bugs': 288, 'build': 289, 'buildings': 290, 'bullet': 291, 'bum': 292, 'bun': 293, 'burglar': 294, 'buried': 295, 'burned': 296, 'burning': 297, 'burns': 298, 'burst': 299, 'bus': 300, 'bush': 301, 'business': 302, 'busy': 303, 'but': 304, 'butter': 305, 'buy': 306, 'by': 307, 'cafe': 308, 'cafeteria': 309, 'cage': 310, 'cake': 311, 'calculation': 312, 'calcutta': 313, 'call': 314, 'called': 315, 'calls': 316, 'came': 317, 'camera': 318, 'can': 319, 'canada': 320, 'cancer': 321, 'cannot': 322, 'cant': 323, 'capital': 324, 'car': 325, 'care': 326, 'careful': 327, 'carefully': 328, 'careless': 329, 'carelessness': 330, 'carried': 331, 'carries': 332, 'cars': 333, 'case': 334, 'cash': 335, 'cat': 336, 'catch': 337, 'catches': 338, 'catching': 339, 'catholic': 340, 'cats': 341, 'caught': 342, 'caused': 343, 'causes': 344, 'cave': 345, 'cease': 346, 'ceased': 347, 'ceiling': 348, 'celebrated': 349, 'cemetery': 350, 'century': 351, 'certain': 352, 'chain': 353, 'chair': 354, 'chairs': 355, 'chance': 356, 'changed': 357, 'changing': 358, 'character': 359, 'charge': 360, 'charged': 361, 'chat': 362, 'chatting': 363, 'cheaper': 364, 'cheek': 365, 'cheers': 366, 'cheese': 367, 'child': 368, 'childhood': 369, 'children': 370, 'china': 371, 'chinese': 372, 'choose': 373, 'chose': 374, 'church': 375, 'cities': 376, 'citizen': 377, 'city': 378, 'class': 379, 'classical': 380, 'classmates': 381, 'classroom': 382, 'clean': 383, 'cleaned': 384, 'cleaning': 385, 'clear': 386, 'cleared': 387, 'clever': 388, 'cliff': 389, 'climate': 390, 'climb': 391, 'climbed': 392, 'clock': 393, 'close': 394, 'closed': 395, 'closing': 396, 'cloth': 397, 'clothes': 398, 'cloud': 399, 'clouds': 400, 'club': 401, 'clung': 402, 'coal': 403, 'coffee': 404, 'coincidentally': 405, 'coins': 406, 'cold': 407, 'colds': 408, 'collecting': 409, 'college': 410, 'color': 411, 'colorful': 412, 'columbus': 413, 'come': 414, 'comes': 415, 'comfort': 416, 'coming': 417, 'command': 418, 'commit': 419, 'committed': 420, 'committee': 421, 'common': 422, 'communicate': 423, 'companies': 424, 'company': 425, 'complained': 426, 'complaining': 427, 'complains': 428, 'completed': 429, 'completely': 430, 'comprehend': 431, 'computer': 432, 'concealed': 433, 'concern': 434, 'concert': 435, 'condition': 436, 'conditioner': 437, 'conduct': 438, 'conference': 439, 'confident': 440, 'conform': 441, 'congratulate': 442, 'congratulations': 443, 'consciousness': 444, 'consented': 445, 'consider': 446, 'consists': 447, 'constantly': 448, 'constitution': 449, 'contact': 450, 'contain': 451, 'contains': 452, 'continent': 453, 'continued': 454, 'contracted': 455, 'control': 456, 'convinced': 457, 'cook': 458, 'cooked': 459, 'cooking': 460, 'copies': 461, 'copyrighted': 462, 'corner': 463, 'correct': 464, 'corruption': 465, 'cost': 466, 'costly': 467, 'coughing': 468, 'could': 469, 'couldnt': 470, 'count': 471, 'counting': 472, 'countries': 473, 'country': 474, 'countryside': 475, 'couple': 476, 'courage': 477, 'cousin': 478, 'cover': 479, 'covered': 480, 'cows': 481, 'cradle': 482, 'crash': 483, 'cricket': 484, 'cried': 485, 'crime': 486, 'cross': 487, 'crossing': 488, 'crow': 489, 'crows': 490, 'crushed': 491, 'cry': 492, 'crying': 493, 'cup': 494, 'cups': 495, 'cured': 496, 'curing': 497, 'current': 498, 'cursed': 499, 'customers': 500, 'cut': 501, 'cute': 502, 'cycling': 503, 'dad': 504, 'daily': 505, 'damp': 506, 'dance': 507, 'danger': 508, 'dangerous': 509, 'dark': 510, 'daughter': 511, 'daughters': 512, 'day': 513, 'days': 514, 'daytime': 515, 'dead': 516, 'deals': 517, 'dear': 518, 'death': 519, 'debt': 520, 'debts': 521, 'decay': 522, 'deceive': 523, 'decide': 524, 'decided': 525, 'decision': 526, 'decline': 527, 'declined': 528, 'deep': 529, 'deeply': 530, 'deer': 531, 'deers': 532, 'defeated': 533, 'definite': 534, 'definitely': 535, 'degree': 536, 'delhi': 537, 'deliberately': 538, 'demanded': 539, 'democracy': 540, 'dentist': 541, 'depends': 542, 'deprived': 543, 'desk': 544, 'destination': 545, 'destined': 546, 'destroyed': 547, 'detail': 548, 'developed': 549, 'devote': 550, 'dialed': 551, 'diamond': 552, 'diamonds': 553, 'dictionary': 554, 'did': 555, 'didnt': 556, 'die': 557, 'died': 558, 'dies': 559, 'differ': 560, 'difference': 561, 'different': 562, 'difficult': 563, 'digging': 564, 'dine': 565, 'dinner': 566, 'directions': 567, 'dirty': 568, 'disappointed': 569, 'disappointment': 570, 'discovered': 571, 'discussed': 572, 'discussing': 573, 'dishes': 574, 'dislike': 575, 'dissatisfied': 576, 'distance': 577, 'distances': 578, 'distinguish': 579, 'distributed': 580, 'disturbed': 581, 'disturbing': 582, 'do': 583, 'doctor': 584, 'does': 585, 'doesnt': 586, 'dog': 587, 'dogs': 588, 'doing': 589, 'doll': 590, 'dollars': 591, 'done': 592, 'dont': 593, 'door': 594, 'doubt': 595, 'dove': 596, 'down': 597, 'dozen': 598, 'dozens': 599, 'drastic': 600, 'drawers': 601, 'dream': 602, 'dreamed': 603, 'dreams': 604, 'drenched': 605, 'dress': 606, 'dressed': 607, 'dried': 608, 'drink': 609, 'drinkable': 610, 'drinking': 611, 'drinks': 612, 'drive': 613, 'driven': 614, 'drivers': 615, 'driving': 616, 'drop': 617, 'dropped': 618, 'drove': 619, 'drug': 620, 'due': 621, 'dues': 622, 'dug': 623, 'during': 624, 'dust': 625, 'each': 626, 'eager': 627, 'ear': 628, 'earlier': 629, 'early': 630, 'earns': 631, 'ears': 632, 'earth': 633, 'earthquake': 634, 'ease': 635, 'easily': 636, 'easter': 637, 'easy': 638, 'eat': 639, 'eaten': 640, 'eating': 641, 'eats': 642, 'economic': 643, 'economies': 644, 'education': 645, 'eggs': 646, 'eight': 647, 'eighteen': 648, 'eightthirty': 649, 'elastic': 650, 'elected': 651, 'election': 652, 'electric': 653, 'electricity': 654, 'elephant': 655, 'elephants': 656, 'elevator': 657, 'eleven': 658, 'else': 659, 'embezzled': 660, 'emergency': 661, 'employees': 662, 'employs': 663, 'empty': 664, 'encouraged': 665, 'end': 666, 'enemies': 667, 'enemy': 668, 'engaged': 669, 'engagement': 670, 'engine': 671, 'england': 672, 'english': 673, 'enjoyed': 674, 'enough': 675, 'enter': 676, 'entered': 677, 'entirely': 678, 'entrance': 679, 'envelope': 680, 'enveloped': 681, 'envied': 682, 'environment': 683, 'equal': 684, 'erase': 685, 'escaped': 686, 'essential': 687, 'europe': 688, 'even': 689, 'eventually': 690, 'ever': 691, 'everest': 692, 'every': 693, 'everybody': 694, 'everyone': 695, 'everything': 696, 'everywhere': 697, 'exactly': 698, 'exaggerated': 699, 'exam': 700, 'examination': 701, 'example': 702, 'excellent': 703, 'except': 704, 'exchange': 705, 'exciting': 706, 'excuse': 707, 'exhibited': 708, 'exist': 709, 'expect': 710, 'expectations': 711, 'expenses': 712, 'expensive': 713, 'experiment': 714, 'explain': 715, 'explained': 716, 'exports': 717, 'extend': 718, 'extent': 719, 'eye': 720, 'eyes': 721, 'face': 722, 'facebook': 723, 'faces': 724, 'fact': 725, 'factory': 726, 'facts': 727, 'fail': 728, 'failed': 729, 'failure': 730, 'fainted': 731, 'fair': 732, 'fairies': 733, 'fall': 734, 'fallen': 735, 'families': 736, 'family': 737, 'famous': 738, 'fantastic': 739, 'far': 740, 'farm': 741, 'farmer': 742, 'farther': 743, 'fast': 744, 'faster': 745, 'fat': 746, 'fate': 747, 'father': 748, 'fathers': 749, 'fault': 750, 'faults': 751, 'favor': 752, 'favorite': 753, 'fear': 754, 'feathers': 755, 'feed': 756, 'feel': 757, 'feeling': 758, 'feelings': 759, 'feels': 760, 'feet': 761, 'fell': 762, 'felt': 763, 'fever': 764, 'few': 765, 'fewer': 766, 'field': 767, 'fifteen': 768, 'fifteenth': 769, 'fifth': 770, 'fifty': 771, 'fight': 772, 'fighting': 773, 'figured': 774, 'film': 775, 'find': 776, 'finding': 777, 'fine': 778, 'fined': 779, 'finger': 780, 'finish': 781, 'finished': 782, 'finland': 783, 'fire': 784, 'fired': 785, 'firm': 786, 'first': 787, 'fish': 788, 'fisherman': 789, 'fishing': 790, 'fit': 791, 'five': 792, 'fixed': 793, 'flames': 794, 'flat': 795, 'flew': 796, 'floor': 797, 'flowers': 798, 'fly': 799, 'flying': 800, 'folder': 801, 'follow': 802, 'followed': 803, 'follows': 804, 'fond': 805, 'food': 806, 'foods': 807, 'fool': 808, 'foot': 809, 'football': 810, 'for': 811, 'forehead': 812, 'foreign': 813, 'foreigners': 814, 'forest': 815, 'forget': 816, 'forgetting': 817, 'forgive': 818, 'forgot': 819, 'form': 820, 'fort': 821, 'fortunate': 822, 'fortune': 823, 'forward': 824, 'found': 825, 'four': 826, 'fox': 827, 'france': 828, 'frankly': 829, 'free': 830, 'freedom': 831, 'french': 832, 'frequently': 833, 'fresh': 834, 'friday': 835, 'friend': 836, 'friends': 837, 'from': 838, 'front': 839, 'fulfill': 840, 'full': 841, 'fully': 842, 'fun': 843, 'funds': 844, 'furnished': 845, 'furniture': 846, 'further': 847, 'future': 848, 'gained': 849, 'game': 850, 'garden': 851, 'gas': 852, 'gate': 853, 'gathered': 854, 'gave': 855, 'gazed': 856, 'gentle': 857, 'gentleman': 858, 'gently': 859, 'genuine': 860, 'george': 861, 'german': 862, 'germany': 863, 'get': 864, 'gets': 865, 'getting': 866, 'ghosts': 867, 'girl': 868, 'girls': 869, 'give': 870, 'given': 871, 'gives': 872, 'glad': 873, 'gladly': 874, 'glass': 875, 'glasses': 876, 'glimpse': 877, 'go': 878, 'god': 879, 'goes': 880, 'going': 881, 'gold': 882, 'golf': 883, 'gone': 884, 'good': 885, 'goodbye': 886, 'goodfornothing': 887, 'goods': 888, 'got': 889, 'governed': 890, 'government': 891, 'granddaughter': 892, 'grandfather': 893, 'grandmother': 894, 'granted': 895, 'grapes': 896, 'grateful': 897, 'grave': 898, 'graveyard': 899, 'great': 900, 'greatest': 901, 'greek': 902, 'greeks': 903, 'green': 904, 'group': 905, 'growing': 906, 'growling': 907, 'grown': 908, 'grows': 909, 'guitar': 910, 'gun': 911, 'gut': 912, 'guy': 913, 'guys': 914, 'gym': 915, 'habit': 916, 'had': 917, 'hair': 918, 'half': 919, 'hammered': 920, 'hand': 921, 'handful': 922, 'handle': 923, 'hands': 924, 'hang': 925, 'happen': 926, 'happened': 927, 'happily': 928, 'happiness': 929, 'happy': 930, 'hard': 931, 'harder': 932, 'hardly': 933, 'hardships': 934, 'harm': 935, 'has': 936, 'hasnt': 937, 'haste': 938, 'hat': 939, 'hate': 940, 'hated': 941, 'hates': 942, 'haunted': 943, 'have': 944, 'havent': 945, 'having': 946, 'haze': 947, 'he': 948, 'head': 949, 'headache': 950, 'headaches': 951, 'health': 952, 'healthy': 953, 'heap': 954, 'hear': 955, 'heard': 956, 'heart': 957, 'heat': 958, 'heavily': 959, 'heavy': 960, 'hell': 961, 'hello': 962, 'helmet': 963, 'help': 964, 'helped': 965, 'helps': 966, 'hens': 967, 'her': 968, 'here': 969, 'hers': 970, 'herself': 971, 'hes': 972, 'hesitate': 973, 'hesitated': 974, 'hesitation': 975, 'hid': 976, 'hiding': 977, 'high': 978, 'higher': 979, 'highest': 980, 'him': 981, 'himself': 982, 'his': 983, 'history': 984, 'hit': 985, 'hobby': 986, 'hokkaido': 987, 'hold': 988, 'holding': 989, 'hole': 990, 'holiday': 991, 'home': 992, 'homework': 993, 'honest': 994, 'honesty': 995, 'hope': 996, 'horn': 997, 'horrifyingly': 998, 'horse': 999, 'hospital': 1000, 'hot': 1001, 'hotel': 1002, 'hour': 1003, 'hours': 1004, 'house': 1005, 'houses': 1006, 'housework': 1007, 'how': 1008, 'hows': 1009, 'huge': 1010, 'hundred': 1011, 'hunger': 1012, 'hungry': 1013, 'hunting': 1014, 'hurried': 1015, 'hurry': 1016, 'hurt': 1017, 'husband': 1018, 'i': 1019, 'id': 1020, 'idea': 1021, 'identify': 1022, 'if': 1023, 'ignorance': 1024, 'ignore': 1025, 'ignored': 1026, 'ill': 1027, 'illegal': 1028, 'illness': 1029, 'im': 1030, 'imagine': 1031, 'imitate': 1032, 'imitation': 1033, 'immediately': 1034, 'impatient': 1035, 'import': 1036, 'important': 1037, 'impossible': 1038, 'in': 1039, 'incident': 1040, 'includes': 1041, 'income': 1042, 'increasing': 1043, 'independence': 1044, 'independent': 1045, 'india': 1046, 'indian': 1047, 'industry': 1048, 'informed': 1049, 'injured': 1050, 'ink': 1051, 'innocence': 1052, 'innocent': 1053, 'innumerable': 1054, 'insecure': 1055, 'insert': 1056, 'inside': 1057, 'inspiration': 1058, 'instant': 1059, 'instead': 1060, 'intelligent': 1061, 'intend': 1062, 'intends': 1063, 'interested': 1064, 'interesting': 1065, 'interests': 1066, 'international': 1067, 'interpret': 1068, 'into': 1069, 'introduce': 1070, 'introduced': 1071, 'investigate': 1072, 'investigated': 1073, 'investigation': 1074, 'investment': 1075, 'invitation': 1076, 'invited': 1077, 'iron': 1078, 'is': 1079, 'island': 1080, 'isnt': 1081, 'it': 1082, 'italy': 1083, 'its': 1084, 'itself': 1085, 'ive': 1086, 'jacket': 1087, 'jam': 1088, 'japan': 1089, 'japanese': 1090, 'jeans': 1091, 'job': 1092, 'jobs': 1093, 'jogging': 1094, 'john': 1095, 'join': 1096, 'joke': 1097, 'journey': 1098, 'joy': 1099, 'jump': 1100, 'jumped': 1101, 'june': 1102, 'jungle': 1103, 'just': 1104, 'keep': 1105, 'keeps': 1106, 'kept': 1107, 'key': 1108, 'keys': 1109, 'kid': 1110, 'kidding': 1111, 'kids': 1112, 'kill': 1113, 'killed': 1114, 'kilo': 1115, 'kind': 1116, 'kindness': 1117, 'king': 1118, 'kite': 1119, 'kites': 1120, 'knew': 1121, 'knife': 1122, 'knocked': 1123, 'knocking': 1124, 'know': 1125, 'known': 1126, 'knows': 1127, 'koala': 1128, 'korea': 1129, 'kyoto': 1130, 'lack': 1131, 'lacks': 1132, 'lady': 1133, 'laid': 1134, 'lake': 1135, 'lakes': 1136, 'land': 1137, 'language': 1138, 'languages': 1139, 'large': 1140, 'larger': 1141, 'largest': 1142, 'last': 1143, 'late': 1144, 'lately': 1145, 'later': 1146, 'laugh': 1147, 'laughed': 1148, 'law': 1149, 'laws': 1150, 'lay': 1151, 'laying': 1152, 'lazy': 1153, 'leading': 1154, 'lean': 1155, 'leaped': 1156, 'learn': 1157, 'learned': 1158, 'learning': 1159, 'least': 1160, 'leather': 1161, 'leave': 1162, 'leaves': 1163, 'leaving': 1164, 'left': 1165, 'leg': 1166, 'legs': 1167, 'lent': 1168, 'less': 1169, 'let': 1170, 'lets': 1171, 'letter': 1172, 'letters': 1173, 'leveled': 1174, 'library': 1175, 'license': 1176, 'lie': 1177, 'life': 1178, 'lift': 1179, 'light': 1180, 'lights': 1181, 'like': 1182, 'liked': 1183, 'likes': 1184, 'limit': 1185, 'lincoln': 1186, 'line': 1187, 'lion': 1188, 'lions': 1189, 'listen': 1190, 'listened': 1191, 'literature': 1192, 'little': 1193, 'live': 1194, 'lived': 1195, 'lives': 1196, 'living': 1197, 'located': 1198, 'locked': 1199, 'london': 1200, 'lonely': 1201, 'long': 1202, 'longer': 1203, 'look': 1204, 'looked': 1205, 'looking': 1206, 'looks': 1207, 'lose': 1208, 'lost': 1209, 'lot': 1210, 'lots': 1211, 'louder': 1212, 'love': 1213, 'loved': 1214, 'loves': 1215, 'loving': 1216, 'low': 1217, 'loyalty': 1218, 'lucky': 1219, 'lunch': 1220, 'lying': 1221, 'machine': 1222, 'made': 1223, 'madman': 1224, 'mahal': 1225, 'maharashtra': 1226, 'maid': 1227, 'mail': 1228, 'main': 1229, 'majority': 1230, 'make': 1231, 'makes': 1232, 'making': 1233, 'malaria': 1234, 'male': 1235, 'man': 1236, 'managed': 1237, 'manager': 1238, 'many': 1239, 'map': 1240, 'market': 1241, 'married': 1242, 'marry': 1243, 'mary': 1244, 'marys': 1245, 'match': 1246, 'matches': 1247, 'materials': 1248, 'math': 1249, 'matsumoto': 1250, 'matter': 1251, 'matters': 1252, 'may': 1253, 'maybe': 1254, 'me': 1255, 'meal': 1256, 'meals': 1257, 'meant': 1258, 'measures': 1259, 'medicine': 1260, 'meet': 1261, 'meeting': 1262, 'member': 1263, 'memory': 1264, 'men': 1265, 'mentioned': 1266, 'message': 1267, 'met': 1268, 'metal': 1269, 'mexico': 1270, 'middle': 1271, 'might': 1272, 'mile': 1273, 'miles': 1274, 'milk': 1275, 'million': 1276, 'minar': 1277, 'mind': 1278, 'mine': 1279, 'minutes': 1280, 'miracle': 1281, 'miss': 1282, 'missed': 1283, 'missing': 1284, 'mist': 1285, 'mistake': 1286, 'mistakes': 1287, 'misunderstanding': 1288, 'mohan': 1289, 'mom': 1290, 'moment': 1291, 'monday': 1292, 'money': 1293, 'monkey': 1294, 'monsoon': 1295, 'month': 1296, 'months': 1297, 'moon': 1298, 'more': 1299, 'morning': 1300, 'most': 1301, 'mother': 1302, 'motioned': 1303, 'motorbike': 1304, 'mountain': 1305, 'mouse': 1306, 'mouth': 1307, 'move': 1308, 'moves': 1309, 'movie': 1310, 'movies': 1311, 'mt': 1312, 'much': 1313, 'mud': 1314, 'mumbai': 1315, 'murder': 1316, 'murdered': 1317, 'murders': 1318, 'museum': 1319, 'music': 1320, 'must': 1321, 'my': 1322, 'myself': 1323, 'mystery': 1324, 'name': 1325, 'named': 1326, 'names': 1327, 'narrow': 1328, 'nationality': 1329, 'naughty': 1330, 'nauseous': 1331, 'near': 1332, 'nearby': 1333, 'nearest': 1334, 'neat': 1335, 'necessary': 1336, 'need': 1337, 'needs': 1338, 'neighborhood': 1339, 'neighbors': 1340, 'neither': 1341, 'nerve': 1342, 'nerves': 1343, 'nests': 1344, 'never': 1345, 'new': 1346, 'news': 1347, 'newspaper': 1348, 'newspapers': 1349, 'next': 1350, 'nextdoor': 1351, 'nice': 1352, 'night': 1353, 'nine': 1354, 'ninth': 1355, 'no': 1356, 'nobody': 1357, 'nobodys': 1358, 'noise': 1359, 'noisy': 1360, 'nonsense': 1361, 'noon': 1362, 'not': 1363, 'notebook': 1364, 'nothing': 1365, 'notice': 1366, 'novel': 1367, 'now': 1368, 'nowadays': 1369, 'number': 1370, 'nun': 1371, 'nuts': 1372, 'object': 1373, 'objected': 1374, 'obliged': 1375, 'observe': 1376, 'observed': 1377, 'obstinate': 1378, 'obtained': 1379, 'obvious': 1380, 'occasionally': 1381, 'occur': 1382, 'occurred': 1383, 'occurs': 1384, 'ocean': 1385, 'oclock': 1386, 'odd': 1387, 'of': 1388, 'off': 1389, 'offer': 1390, 'offered': 1391, 'office': 1392, 'officer': 1393, 'often': 1394, 'oh': 1395, 'oil': 1396, 'ok': 1397, 'old': 1398, 'older': 1399, 'on': 1400, 'once': 1401, 'one': 1402, 'ones': 1403, 'only': 1404, 'open': 1405, 'opened': 1406, 'opening': 1407, 'operate': 1408, 'operations': 1409, 'opinion': 1410, 'opinions': 1411, 'opponent': 1412, 'opportunity': 1413, 'oppose': 1414, 'opposite': 1415, 'or': 1416, 'oranges': 1417, 'order': 1418, 'ordered': 1419, 'originally': 1420, 'orphan': 1421, 'osaka': 1422, 'other': 1423, 'others': 1424, 'ought': 1425, 'our': 1426, 'ours': 1427, 'ourselves': 1428, 'out': 1429, 'outside': 1430, 'over': 1431, 'overseas': 1432, 'owe': 1433, 'own': 1434, 'owner': 1435, 'oxford': 1436, 'pack': 1437, 'page': 1438, 'paid': 1439, 'pain': 1440, 'paint': 1441, 'painted': 1442, 'pair': 1443, 'paper': 1444, 'parallel': 1445, 'parcel': 1446, 'pardon': 1447, 'parents': 1448, 'paris': 1449, 'park': 1450, 'parking': 1451, 'part': 1452, 'parts': 1453, 'party': 1454, 'pass': 1455, 'passed': 1456, 'passengers': 1457, 'passport': 1458, 'past': 1459, 'patience': 1460, 'patient': 1461, 'patrolling': 1462, 'pay': 1463, 'peace': 1464, 'peacefully': 1465, 'peacock': 1466, 'peak': 1467, 'peanuts': 1468, 'peasant': 1469, 'pen': 1470, 'penalty': 1471, 'pencil': 1472, 'pencils': 1473, 'people': 1474, 'peoples': 1475, 'percent': 1476, 'perfect': 1477, 'perhaps': 1478, 'person': 1479, 'persuade': 1480, 'pet': 1481, 'phone': 1482, 'photocopier': 1483, 'photograph': 1484, 'phrase': 1485, 'phrases': 1486, 'physics': 1487, 'pianist': 1488, 'piano': 1489, 'pick': 1490, 'pickpocketing': 1491, 'picture': 1492, 'pictures': 1493, 'pie': 1494, 'piece': 1495, 'pieces': 1496, 'pigeons': 1497, 'pink': 1498, 'pipe': 1499, 'pity': 1500, 'place': 1501, 'plan': 1502, 'plane': 1503, 'plans': 1504, 'planted': 1505, 'planting': 1506, 'plants': 1507, 'play': 1508, 'player': 1509, 'playing': 1510, 'plays': 1511, 'please': 1512, 'pleased': 1513, 'plenty': 1514, 'plus': 1515, 'pm': 1516, 'pocket': 1517, 'poem': 1518, 'poet': 1519, 'point': 1520, 'poisonous': 1521, 'pole': 1522, 'police': 1523, 'policeman': 1524, 'policy': 1525, 'politics': 1526, 'pollute': 1527, 'polluted': 1528, 'pond': 1529, 'poor': 1530, 'poorer': 1531, 'popular': 1532, 'population': 1533, 'possession': 1534, 'possible': 1535, 'potatoes': 1536, 'pounds': 1537, 'poured': 1538, 'pouring': 1539, 'pours': 1540, 'power': 1541, 'powers': 1542, 'practice': 1543, 'prattles': 1544, 'precious': 1545, 'prefer': 1546, 'prefers': 1547, 'prepare': 1548, 'prepared': 1549, 'preparing': 1550, 'present': 1551, 'presents': 1552, 'president': 1553, 'pressed': 1554, 'pretending': 1555, 'pretty': 1556, 'prevented': 1557, 'price': 1558, 'pride': 1559, 'prided': 1560, 'prison': 1561, 'prisoner': 1562, 'prisoners': 1563, 'private': 1564, 'prize': 1565, 'problem': 1566, 'problems': 1567, 'products': 1568, 'profession': 1569, 'profit': 1570, 'program': 1571, 'promise': 1572, 'promised': 1573, 'promote': 1574, 'prompt': 1575, 'pronounce': 1576, 'properly': 1577, 'property': 1578, 'proposal': 1579, 'protect': 1580, 'protection': 1581, 'proud': 1582, 'prove': 1583, 'proved': 1584, 'proverb': 1585, 'public': 1586, 'pulled': 1587, 'pulse': 1588, 'pumpkin': 1589, 'punished': 1590, 'purchases': 1591, 'purse': 1592, 'put': 1593, 'quality': 1594, 'question': 1595, 'questions': 1596, 'quick': 1597, 'quickly': 1598, 'quit': 1599, 'quite': 1600, 'quitting': 1601, 'quota': 1602, 'qutub': 1603, 'rabbit': 1604, 'race': 1605, 'radio': 1606, 'rain': 1607, 'rainbow': 1608, 'raining': 1609, 'rains': 1610, 'rainy': 1611, 'raise': 1612, 'raised': 1613, 'ran': 1614, 'rang': 1615, 'rarely': 1616, 'rather': 1617, 'reach': 1618, 'reached': 1619, 'read': 1620, 'reading': 1621, 'reads': 1622, 'ready': 1623, 'real': 1624, 'realize': 1625, 'realized': 1626, 'really': 1627, 'reap': 1628, 'reason': 1629, 'reasonable': 1630, 'rebellion': 1631, 'receive': 1632, 'received': 1633, 'recognize': 1634, 'recognized': 1635, 'red': 1636, 'reduce': 1637, 'reform': 1638, 'reforming': 1639, 'refund': 1640, 'refused': 1641, 'regard': 1642, 'regret': 1643, 'release': 1644, 'released': 1645, 'rely': 1646, 'remain': 1647, 'remained': 1648, 'remains': 1649, 'remember': 1650, 'reminded': 1651, 'reminds': 1652, 'remorse': 1653, 'remote': 1654, 'remove': 1655, 'removing': 1656, 'repair': 1657, 'repairing': 1658, 'repeated': 1659, 'reply': 1660, 'report': 1661, 'reproached': 1662, 'requires': 1663, 'rescued': 1664, 'resembles': 1665, 'resolved': 1666, 'respect': 1667, 'respected': 1668, 'responsible': 1669, 'rest': 1670, 'rested': 1671, 'resting': 1672, 'result': 1673, 'results': 1674, 'return': 1675, 'returning': 1676, 'reverse': 1677, 'rice': 1678, 'rich': 1679, 'richest': 1680, 'rid': 1681, 'ride': 1682, 'right': 1683, 'ringing': 1684, 'risks': 1685, 'river': 1686, 'road': 1687, 'robbed': 1688, 'rodicas': 1689, 'rome': 1690, 'roof': 1691, 'room': 1692, 'rooms': 1693, 'rope': 1694, 'roses': 1695, 'rotates': 1696, 'rotten': 1697, 'round': 1698, 'rubber': 1699, 'rude': 1700, 'rudeness': 1701, 'rule': 1702, 'rules': 1703, 'rumor': 1704, 'run': 1705, 'rung': 1706, 'running': 1707, 'russian': 1708, 's': 1709, 'sacred': 1710, 'sad': 1711, 'sadness': 1712, 'safety': 1713, 'said': 1714, 'sail': 1715, 'salary': 1716, 'sale': 1717, 'salt': 1718, 'same': 1719, 'sand': 1720, 'sandwiches': 1721, 'sang': 1722, 'sat': 1723, 'satisfied': 1724, 'satisfying': 1725, 'save': 1726, 'saved': 1727, 'saver': 1728, 'saw': 1729, 'say': 1730, 'saying': 1731, 'says': 1732, 'scale': 1733, 'scar': 1734, 'scared': 1735, 'scattered': 1736, 'school': 1737, 'schools': 1738, 'science': 1739, 'scissors': 1740, 'scolded': 1741, 'sea': 1742, 'search': 1743, 'searching': 1744, 'season': 1745, 'seat': 1746, 'seats': 1747, 'secret': 1748, 'see': 1749, 'seeds': 1750, 'seeing': 1751, 'seeking': 1752, 'seem': 1753, 'seemed': 1754, 'seems': 1755, 'seen': 1756, 'seicho': 1757, 'seldom': 1758, 'selfish': 1759, 'send': 1760, 'sense': 1761, 'sent': 1762, 'sentence': 1763, 'sentenced': 1764, 'sentences': 1765, 'servant': 1766, 'services': 1767, 'set': 1768, 'sets': 1769, 'settle': 1770, 'seven': 1771, 'seventeen': 1772, 'seventh': 1773, 'several': 1774, 'shade': 1775, 'shall': 1776, 'shared': 1777, 'sharp': 1778, 'sharpshooter': 1779, 'shaving': 1780, 'she': 1781, 'sheep': 1782, 'sheer': 1783, 'sheets': 1784, 'shes': 1785, 'ship': 1786, 'shirt': 1787, 'shirts': 1788, 'shoes': 1789, 'shooting': 1790, 'shop': 1791, 'short': 1792, 'shot': 1793, 'should': 1794, 'shoulder': 1795, 'shouldnt': 1796, 'shouldve': 1797, 'shout': 1798, 'shouted': 1799, 'show': 1800, 'showed': 1801, 'shower': 1802, 'shows': 1803, 'shrank': 1804, 'shut': 1805, 'sick': 1806, 'sickness': 1807, 'sight': 1808, 'sign': 1809, 'silent': 1810, 'silk': 1811, 'silky': 1812, 'similar': 1813, 'simple': 1814, 'sin': 1815, 'since': 1816, 'sing': 1817, 'singing': 1818, 'single': 1819, 'sinking': 1820, 'sister': 1821, 'sisters': 1822, 'sit': 1823, 'situation': 1824, 'six': 1825, 'sixteenth': 1826, 'sixth': 1827, 'sixty': 1828, 'size': 1829, 'skating': 1830, 'skies': 1831, 'skin': 1832, 'sky': 1833, 'slap': 1834, 'slave': 1835, 'sleep': 1836, 'sleeping': 1837, 'sleepy': 1838, 'slept': 1839, 'slow': 1840, 'slowly': 1841, 'small': 1842, 'smell': 1843, 'smelled': 1844, 'smelling': 1845, 'smelt': 1846, 'smile': 1847, 'smiled': 1848, 'smog': 1849, 'smoke': 1850, 'smoking': 1851, 'smooth': 1852, 'smuggler': 1853, 'snake': 1854, 'snakes': 1855, 'snow': 1856, 'snowing': 1857, 'so': 1858, 'soccer': 1859, 'socks': 1860, 'sofa': 1861, 'soft': 1862, 'softer': 1863, 'sold': 1864, 'soldiers': 1865, 'solve': 1866, 'solved': 1867, 'some': 1868, 'someday': 1869, 'someone': 1870, 'something': 1871, 'sometimes': 1872, 'somewhere': 1873, 'son': 1874, 'song': 1875, 'soon': 1876, 'sooner': 1877, 'sorry': 1878, 'sort': 1879, 'sound': 1880, 'soup': 1881, 'sour': 1882, 'source': 1883, 'sources': 1884, 'south': 1885, 'sow': 1886, 'spa': 1887, 'spanish': 1888, 'speak': 1889, 'speaking': 1890, 'speaks': 1891, 'specialty': 1892, 'speech': 1893, 'speed': 1894, 'spend': 1895, 'spending': 1896, 'spends': 1897, 'spent': 1898, 'spit': 1899, 'spoil': 1900, 'spoiled': 1901, 'spoken': 1902, 'spread': 1903, 'squirrel': 1904, 'st': 1905, 'stairs': 1906, 'stamps': 1907, 'stand': 1908, 'standing': 1909, 'staring': 1910, 'stars': 1911, 'start': 1912, 'started': 1913, 'starting': 1914, 'startled': 1915, 'starts': 1916, 'state': 1917, 'statement': 1918, 'states': 1919, 'station': 1920, 'stay': 1921, 'stayed': 1922, 'staying': 1923, 'steal': 1924, 'sticking': 1925, 'still': 1926, 'stingy': 1927, 'stole': 1928, 'stolen': 1929, 'stomach': 1930, 'stomachache': 1931, 'stone': 1932, 'stones': 1933, 'stood': 1934, 'stop': 1935, 'stopped': 1936, 'store': 1937, 'stories': 1938, 'storm': 1939, 'story': 1940, 'straight': 1941, 'strange': 1942, 'stranger': 1943, 'strangers': 1944, 'street': 1945, 'strike': 1946, 'string': 1947, 'stripped': 1948, 'strong': 1949, 'struck': 1950, 'stubborn': 1951, 'stuck': 1952, 'student': 1953, 'students': 1954, 'studied': 1955, 'study': 1956, 'studying': 1957, 'stuff': 1958, 'stuttering': 1959, 'subject': 1960, 'subway': 1961, 'succeed': 1962, 'succeeded': 1963, 'success': 1964, 'successful': 1965, 'such': 1966, 'sudden': 1967, 'suddenly': 1968, 'suffers': 1969, 'sugar': 1970, 'suicide': 1971, 'suit': 1972, 'suitcase': 1973, 'summer': 1974, 'sun': 1975, 'sunday': 1976, 'sundays': 1977, 'superior': 1978, 'supermarket': 1979, 'supper': 1980, 'supposed': 1981, 'sure': 1982, 'surprise': 1983, 'surprised': 1984, 'swallow': 1985, 'swear': 1986, 'sweat': 1987, 'sweater': 1988, 'swim': 1989, 'swimming': 1990, 'switch': 1991, 'switzerland': 1992, 'swollen': 1993, 'table': 1994, 'tablets': 1995, 'tail': 1996, 'taiwan': 1997, 'taj': 1998, 'take': 1999, 'taken': 2000, 'takes': 2001, 'taking': 2002, 'talk': 2003, 'talked': 2004, 'talking': 2005, 'talks': 2006, 'tall': 2007, 'tame': 2008, 'taste': 2009, 'tastes': 2010, 'taught': 2011, 'tax': 2012, 'taxi': 2013, 'tea': 2014, 'teach': 2015, 'teacher': 2016, 'teachers': 2017, 'teaching': 2018, 'team': 2019, 'tears': 2020, 'teeth': 2021, 'telephone': 2022, 'television': 2023, 'tell': 2024, 'temper': 2025, 'temperature': 2026, 'temples': 2027, 'ten': 2028, 'tends': 2029, 'tennis': 2030, 'teresa': 2031, 'terminal': 2032, 'test': 2033, 'thames': 2034, 'than': 2035, 'thank': 2036, 'that': 2037, 'thats': 2038, 'the': 2039, 'theater': 2040, 'theft': 2041, 'their': 2042, 'them': 2043, 'themselves': 2044, 'then': 2045, 'theory': 2046, 'there': 2047, 'theres': 2048, 'these': 2049, 'they': 2050, 'thick': 2051, 'thief': 2052, 'thieves': 2053, 'thin': 2054, 'thing': 2055, 'things': 2056, 'think': 2057, 'thinking': 2058, 'thinks': 2059, 'thirsty': 2060, 'thirty': 2061, 'this': 2062, 'those': 2063, 'thought': 2064, 'thousands': 2065, 'three': 2066, 'through': 2067, 'throughout': 2068, 'throw': 2069, 'throwing': 2070, 'thrown': 2071, 'ticket': 2072, 'tickets': 2073, 'tidy': 2074, 'tie': 2075, 'tiger': 2076, 'till': 2077, 'time': 2078, 'times': 2079, 'tiny': 2080, 'tired': 2081, 'to': 2082, 'tobacco': 2083, 'today': 2084, 'toes': 2085, 'together': 2086, 'toilet': 2087, 'tokyo': 2088, 'told': 2089, 'tolerate': 2090, 'tom': 2091, 'tomorrow': 2092, 'toms': 2093, 'tonight': 2094, 'too': 2095, 'took': 2096, 'top': 2097, 'tore': 2098, 'touch': 2099, 'touching': 2100, 'toudaiji': 2101, 'toward': 2102, 'towel': 2103, 'towels': 2104, 'town': 2105, 'towns': 2106, 'toy': 2107, 'toyota': 2108, 'tracks': 2109, 'trade': 2110, 'traffic': 2111, 'train': 2112, 'traitor': 2113, 'translate': 2114, 'travel': 2115, 'travels': 2116, 'treasure': 2117, 'treats': 2118, 'tree': 2119, 'trees': 2120, 'tried': 2121, 'tries': 2122, 'trip': 2123, 'trouble': 2124, 'true': 2125, 'truly': 2126, 'trust': 2127, 'truth': 2128, 'try': 2129, 'trying': 2130, 'tuesday': 2131, 'turn': 2132, 'turned': 2133, 'turning': 2134, 'tv': 2135, 'twenty': 2136, 'twentyfour': 2137, 'twice': 2138, 'twin': 2139, 'twins': 2140, 'two': 2141, 'umbrella': 2142, 'unbelievable': 2143, 'uncle': 2144, 'under': 2145, 'underlined': 2146, 'understand': 2147, 'understands': 2148, 'undone': 2149, 'unfounded': 2150, 'unhappy': 2151, 'unicycle': 2152, 'unites': 2153, 'university': 2154, 'unless': 2155, 'unsolved': 2156, 'until': 2157, 'unused': 2158, 'up': 2159, 'upon': 2160, 'us': 2161, 'use': 2162, 'used': 2163, 'useful': 2164, 'usual': 2165, 'usually': 2166, 'vain': 2167, 'value': 2168, 'vary': 2169, 'vase': 2170, 'vegetables': 2171, 'ventured': 2172, 'veracity': 2173, 'vertical': 2174, 'very': 2175, 'victory': 2176, 'village': 2177, 'visit': 2178, 'visited': 2179, 'visiting': 2180, 'vital': 2181, 'vocabulary': 2182, 'voice': 2183, 'volume': 2184, 'vomiting': 2185, 'voted': 2186, 'voyage': 2187, 'waist': 2188, 'wait': 2189, 'waited': 2190, 'waiting': 2191, 'wake': 2192, 'waking': 2193, 'walk': 2194, 'walked': 2195, 'walking': 2196, 'walks': 2197, 'wall': 2198, 'wallet': 2199, 'want': 2200, 'wanted': 2201, 'wants': 2202, 'war': 2203, 'wares': 2204, 'warned': 2205, 'wars': 2206, 'was': 2207, 'wash': 2208, 'washed': 2209, 'washing': 2210, 'washington': 2211, 'wasnt': 2212, 'waste': 2213, 'watch': 2214, 'watched': 2215, 'watching': 2216, 'water': 2217, 'watering': 2218, 'way': 2219, 'we': 2220, 'wealthy': 2221, 'wear': 2222, 'wearing': 2223, 'wears': 2224, 'weather': 2225, 'wed': 2226, 'wedding': 2227, 'week': 2228, 'weekend': 2229, 'weeks': 2230, 'weighed': 2231, 'weird': 2232, 'welcome': 2233, 'well': 2234, 'went': 2235, 'were': 2236, 'werent': 2237, 'west': 2238, 'western': 2239, 'weve': 2240, 'what': 2241, 'whatever': 2242, 'whatll': 2243, 'whatre': 2244, 'whats': 2245, 'wheel': 2246, 'when': 2247, 'where': 2248, 'wheres': 2249, 'wherever': 2250, 'whether': 2251, 'which': 2252, 'whichever': 2253, 'while': 2254, 'white': 2255, 'who': 2256, 'whoever': 2257, 'whole': 2258, 'whom': 2259, 'whos': 2260, 'whose': 2261, 'why': 2262, 'widely': 2263, 'wife': 2264, 'wild': 2265, 'will': 2266, 'willing': 2267, 'win': 2268, 'wind': 2269, 'window': 2270, 'windows': 2271, 'wine': 2272, 'wings': 2273, 'wins': 2274, 'winter': 2275, 'wipe': 2276, 'wiped': 2277, 'wish': 2278, 'wishes': 2279, 'with': 2280, 'withered': 2281, 'within': 2282, 'without': 2283, 'witnessed': 2284, 'woke': 2285, 'woman': 2286, 'women': 2287, 'won': 2288, 'wonder': 2289, 'wonderful': 2290, 'wondering': 2291, 'wonders': 2292, 'wont': 2293, 'wood': 2294, 'wooden': 2295, 'woods': 2296, 'wool': 2297, 'word': 2298, 'words': 2299, 'wore': 2300, 'work': 2301, 'worked': 2302, 'workers': 2303, 'working': 2304, 'works': 2305, 'world': 2306, 'worlds': 2307, 'worry': 2308, 'worrying': 2309, 'worse': 2310, 'worst': 2311, 'worthless': 2312, 'would': 2313, 'wouldnt': 2314, 'wow': 2315, 'write': 2316, 'writing': 2317, 'written': 2318, 'wrong': 2319, 'wrote': 2320, 'year': 2321, 'years': 2322, 'yen': 2323, 'yes': 2324, 'yesterday': 2325, 'yet': 2326, 'yield': 2327, 'york': 2328, 'you': 2329, 'youd': 2330, 'youll': 2331, 'young': 2332, 'younger': 2333, 'youngster': 2334, 'your': 2335, 'youre': 2336, 'yours': 2337, 'yourself': 2338, 'youth': 2339, 'youve': 2340, 'zoo': 2341}\n",
      "{'START_': 1, '_END': 2, 'a': 3, 'b': 4, 'i': 5, 'अँडे': 6, 'अंकल': 7, 'अंगर': 8, 'अंगूर': 9, 'अंग्रेज़ी': 10, 'अंग्रेजी': 11, 'अंडे': 12, 'अंत': 13, 'अंतर': 14, 'अंतर्राष्ट्रीय': 15, 'अंदर': 16, 'अंदेखा': 17, 'अंधेर': 18, 'अकल': 19, 'अकलमंद': 20, 'अकेला': 21, 'अकेलापन': 22, 'अकेले': 23, 'अक्खड़पन': 24, 'अक्सर': 25, 'अखबार': 26, 'अखबारों': 27, 'अगर': 28, 'अगला': 29, 'अगली': 30, 'अगले': 31, 'अगस्त': 32, 'अचानक': 33, 'अच्छा': 34, 'अच्छाई': 35, 'अच्छा।': 36, 'अच्छी': 37, 'अच्छे': 38, 'अच्छेपन': 39, 'अजनबियों': 40, 'अजनबी': 41, 'अजीब': 42, 'अज्ञान': 43, 'अटक': 44, 'अटैची': 45, 'अठारह': 46, 'अड़ियल': 47, 'अड्डा': 48, 'अड्डे': 49, 'अद्भुत': 50, 'अधिकांश': 51, 'अधिकारियों': 52, 'अध्यापक': 53, 'अनदेखा': 54, 'अनहोनी': 55, 'अनाथ': 56, 'अनिवार्य': 57, 'अनीश्वरवादी': 58, 'अनुपस्थित': 59, 'अनुवाद': 60, 'अपना': 61, 'अपनी': 62, 'अपने': 63, 'अपनेआप': 64, 'अपौइंटमेंट': 65, 'अप्रैल': 66, 'अफ़वाह': 67, 'अफ़सर': 68, 'अफ़्रीका': 69, 'अब': 70, 'अबसे': 71, 'अभ': 72, 'अभिनेता': 73, 'अभिनेत्री': 74, 'अभिमान': 75, 'अभिमानी': 76, 'अभिलाषा': 77, 'अभिलाषाओं': 78, 'अभी': 79, 'अभीअभी': 80, 'अमरीका': 81, 'अमीर': 82, 'अमेरिका': 83, 'अम्रीका': 84, 'अम्रीकी': 85, 'अरब': 86, 'अरबी': 87, 'अरे': 88, 'अर्थ': 89, 'अर्थतंत्रों': 90, 'अर्थव्यवस्था': 91, 'अलग': 92, 'अलगअलग': 93, 'अलावा': 94, 'अविश्वसनीय': 95, 'असभ्यता': 96, 'असर': 97, 'असली': 98, 'असुरक्षित': 99, 'अस्तित्व': 100, 'अस्पताल': 101, 'अस्वीकार': 102, 'अहंकारी': 103, 'अौर': 104, 'आ': 105, 'आँख': 106, 'आँखे': 107, 'आँखें': 108, 'आँखों': 109, 'आँसुओं': 110, 'आंकरेज': 111, 'आइडिया': 112, 'आई': 113, 'आईं।': 114, 'आईएगा।': 115, 'आईए।': 116, 'आई।': 117, 'आऊँगा।': 118, 'आऊंगा।': 119, 'आऊट': 120, 'आए': 121, 'आएँ': 122, 'आएँगे': 123, 'आएँ।': 124, 'आएंगे': 125, 'आएगा': 126, 'आएगा।': 127, 'आएगी': 128, 'आएगी।': 129, 'आए।': 130, 'आओ': 131, 'आओगे': 132, 'आओ।': 133, 'आकर': 134, 'आकार': 135, 'आखरी': 136, 'आखिरकार': 137, 'आखें': 138, 'आग': 139, 'आगे': 140, 'आज': 141, 'आजकल': 142, 'आज़माती': 143, 'आज़ादी': 144, 'आजाओ।': 145, 'आठ': 146, 'आता': 147, 'आता।': 148, 'आती': 149, 'आते': 150, 'आत्महत्या': 151, 'आदत': 152, 'आदमियों': 153, 'आदमी': 154, 'आदरसम्मान': 155, 'आदेश': 156, 'आधा': 157, 'आधी': 158, 'आधे': 159, 'आना': 160, 'आना।': 161, 'आने': 162, 'आप': 163, 'आपका': 164, 'आपकी': 165, 'आपके': 166, 'आपको': 167, 'आपत्ति': 168, 'आपत्ती': 169, 'आपने': 170, 'आपसे': 171, 'आपातकालीन': 172, 'आबादी': 173, 'आभारी': 174, 'आम': 175, 'आमंत्रण': 176, 'आमतौर': 177, 'आमदनी': 178, 'आमनेसामने': 179, 'आयडिया': 180, 'आया': 181, 'आयात': 182, 'आया।': 183, 'आराम': 184, 'आर्ट': 185, 'आलसी': 186, 'आलूओं': 187, 'आवश्यक': 188, 'आवाज़': 189, 'आशंका': 190, 'आशा': 191, 'आश्चर्यों': 192, 'आसपड़ोस': 193, 'आसपास': 194, 'आसमान': 195, 'आसान': 196, 'आसानी': 197, 'आस्ट्रेलिया': 198, 'इंग्लैंड': 199, 'इंग्लैड': 200, 'इंजन': 201, 'इंतेज़ार': 202, 'इकट्ठा': 203, 'इक्कीस': 204, 'इगज़ैम': 205, 'इच्छा': 206, 'इटली': 207, 'इतना': 208, 'इतनी': 209, 'इतने': 210, 'इतिहास': 211, 'इत्तेफ़ाक': 212, 'इन': 213, 'इनकार': 214, 'इनके': 215, 'इनवेस्टमेंट': 216, 'इनसान': 217, 'इनाम': 218, 'इन्तज़ार': 219, 'इन्द्रधनुष': 220, 'इमारतों': 221, 'इरादा': 222, 'इलज़ाम': 223, 'इलाज': 224, 'इशारा': 225, 'इस': 226, 'इसका': 227, 'इसके': 228, 'इसको': 229, 'इसमें': 230, 'इसलिए': 231, 'इससे': 232, 'इसीलिए': 233, 'इसे': 234, 'इस्तेमाल': 235, 'ईनाम': 236, 'ईमानदार': 237, 'ईमानदारी': 238, 'ईश्वर': 239, 'उगाई': 240, 'उछल': 241, 'उछलती': 242, 'उछलो': 243, 'उठकर': 244, 'उठता': 245, 'उठते': 246, 'उठना': 247, 'उठा': 248, 'उठाती': 249, 'उठाना': 250, 'उठाने': 251, 'उठाया।': 252, 'उठा।': 253, 'उठी।': 254, 'उड़': 255, 'उड़कर': 256, 'उड़ते': 257, 'उड़ना': 258, 'उड़ा': 259, 'उड़ाई।': 260, 'उड़ाएँ': 261, 'उड़ाओ।': 262, 'उड़ाता': 263, 'उड़ातें': 264, 'उड़ान': 265, 'उड़ाना': 266, 'उड़ाने': 267, 'उड़ाया।': 268, 'उतना': 269, 'उतनी': 270, 'उतर': 271, 'उतरेगा।': 272, 'उतार': 273, 'उतारने': 274, 'उतारा': 275, 'उतारिए।': 276, 'उतारे': 277, 'उतारे।': 278, 'उत्तम': 279, 'उत्तर': 280, 'उत्तीर्ण': 281, 'उत्पाद': 282, 'उदाहरण': 283, 'उद्देश्य': 284, 'उद्योग': 285, 'उधार': 286, 'उन': 287, 'उनका': 288, 'उनकी': 289, 'उनके': 290, 'उनको': 291, 'उनपर': 292, 'उनमें': 293, 'उनसे': 294, 'उन्हे': 295, 'उन्हें': 296, 'उन्होंने': 297, 'उपन्यास': 298, 'उपयोग': 299, 'उपयोगी': 300, 'उपलब्ध': 301, 'उम्मीद': 302, 'उम्मीदों': 303, 'उम्र': 304, 'उलटा': 305, 'उलटी': 306, 'उल्टी': 307, 'उस': 308, 'उसका': 309, 'उसकी': 310, 'उसके': 311, 'उसको': 312, 'उसने': 313, 'उसपर': 314, 'उसमें': 315, 'उससे': 316, 'उसी': 317, 'उसे': 318, 'ऊँगलियाँ': 319, 'ऊँगली': 320, 'ऊँचा': 321, 'ऊँची': 322, 'ऊन': 323, 'ऊपर': 324, 'ऋतु': 325, 'एक': 326, 'एकदम': 327, 'एकदूसरे': 328, 'एकदो': 329, 'एकसमान': 330, 'एकसाथ': 331, 'एड्स': 332, 'एयरपोर्ट': 333, 'एलर्जी': 334, 'एवरेस्ट': 335, 'एशिया': 336, 'एसटर': 337, 'एसी': 338, 'एहसान': 339, 'ऐक्सीडेंट': 340, 'ऐनक': 341, 'ऐमब्यूलेंस': 342, 'ऐसा': 343, 'ऐसी': 344, 'ऐसे': 345, 'ऑक्सफ़र्ड': 346, 'ऑर्डर': 347, 'ऑस्ट्रेलिया': 348, 'ओर': 349, 'ओसाका': 350, 'और': 351, 'औरडर': 352, 'औरत': 353, 'औरतों': 354, 'औरतोऔर': 355, 'औरभीऔर': 356, 'कंजूस': 357, 'कंधे': 358, 'कंबल': 359, 'कई': 360, 'कक्षा': 361, 'कखगध': 362, 'कटवाता': 363, 'कटवाना': 364, 'कटाने': 365, 'कटोरियाँ': 366, 'कठिन': 367, 'कठोर': 368, 'कड़वा': 369, 'कड़वी': 370, 'कत्ल': 371, 'कदम': 372, 'कद्दू': 373, 'कनाडा': 374, 'कप': 375, 'कपड़ा': 376, 'कपड़े': 377, 'कपड़ो': 378, 'कपों': 379, 'कब': 380, 'कबसे': 381, 'कबूतरों': 382, 'कब्रिस्तान': 383, 'कभी': 384, 'कभीकभार': 385, 'कभीकभी': 386, 'कम': 387, 'कमर': 388, 'कमरा': 389, 'कमरे': 390, 'कमसेकम': 391, 'कमा': 392, 'कमाई': 393, 'कमाता': 394, 'कमी': 395, 'कमीज़': 396, 'कमीज़ें': 397, 'कम्पनियाँ': 398, 'कम्पनी': 399, 'कम्प्यूटर': 400, 'कयोंकि': 401, 'कर': 402, 'करके': 403, 'करता': 404, 'करता।': 405, 'करती': 406, 'करतीं': 407, 'करती।': 408, 'करते': 409, 'करतें': 410, 'करदिआ।': 411, 'करदी।': 412, 'करदें': 413, 'करदो': 414, 'करदोगे': 415, 'करदो।': 416, 'करना': 417, 'करना।': 418, 'करनी': 419, 'करनीं': 420, 'करने': 421, 'करलें।': 422, 'करवाई।': 423, 'करवाना': 424, 'करवाया': 425, 'करवाया।': 426, 'करा': 427, 'करी': 428, 'करीं।': 429, 'करी।': 430, 'करूँ': 431, 'करूँगा': 432, 'करूँगा।': 433, 'करूँगी।': 434, 'करूँ।': 435, 'करूंगी।': 436, 'करेंगे': 437, 'करेंगे।': 438, 'करें।': 439, 'करेगा': 440, 'करेगी': 441, 'करो': 442, 'करोगे': 443, 'करो।': 444, 'कर्ज़': 445, 'कर्मचारियों': 446, 'कल': 447, 'कलकत्ता': 448, 'कलम': 449, 'कलाकार': 450, 'कवि': 451, 'कविता': 452, 'कष्ट': 453, 'कसम': 454, 'कह': 455, 'कहता': 456, 'कहते': 457, 'कहना': 458, 'कहने': 459, 'कहा': 460, 'कहाँ': 461, 'कहां': 462, 'कहानियाँ': 463, 'कहानी': 464, 'कहावत': 465, 'कहा।': 466, 'कहीं': 467, 'कहूँ।': 468, 'कहे': 469, 'कहें': 470, 'कहेंगे।': 471, 'क़त्ल': 472, 'क़ब्र': 473, 'क़ातिल': 474, 'क़ानून': 475, 'क़ुतुब': 476, 'क़ैदियों': 477, 'क़ैदी': 478, 'का': 479, 'कागज़': 480, 'काट': 481, 'काटकर': 482, 'काटते': 483, 'काटोगे।': 484, 'काटो।': 485, 'कान': 486, 'कानून': 487, 'काफ़ी': 488, 'काम': 489, 'कामकाज': 490, 'कामयाब': 491, 'कामयाबी': 492, 'कारण': 493, 'कारोबार': 494, 'कार्यक्रम': 495, 'काला': 496, 'कालीं': 497, 'काले': 498, 'काश': 499, 'कि': 500, 'किजिए': 501, 'कितना': 502, 'कितनी': 503, 'कितने': 504, 'किता': 505, 'किताब': 506, 'किताबें': 507, 'किताबों': 508, 'किनारे': 509, 'किया': 510, 'किया।': 511, 'कियेधरे': 512, 'किला': 513, 'किलो': 514, 'किस': 515, 'किसकी': 516, 'किसके': 517, 'किसको': 518, 'किसने': 519, 'किसलिए': 520, 'किसान': 521, 'किसी': 522, 'किसे': 523, 'की': 524, 'कीचड़': 525, 'कीजिए': 526, 'कीजिएगा': 527, 'कीजिएगा।': 528, 'कीजिए।': 529, 'कीड़ों': 530, 'कीमत': 531, 'कीमती': 532, 'कीमतें': 533, 'की।': 534, 'कुचल': 535, 'कुछ': 536, 'कुछनकुछ': 537, 'कुत्ता': 538, 'कुत्ते': 539, 'कुत्ते।': 540, 'कुरसियाँ': 541, 'कुरसी': 542, 'कूद': 543, 'कूदो': 544, 'कूदो।': 545, 'कृपया': 546, 'के': 547, 'केक': 548, 'केवल': 549, 'केस': 550, 'कैंचियों': 551, 'कैंसर': 552, 'कैथोलिक': 553, 'कैनटीन': 554, 'कैफ़े': 555, 'कैमरा': 556, 'कैसा': 557, 'कैसी': 558, 'कैसे': 559, 'कॉनसर्ट': 560, 'कॉपियाँ': 561, 'कॉपीराईट': 562, 'कॉफ़ी': 563, 'कॉफ़ी।': 564, 'कॉल': 565, 'कॉलेज': 566, 'को': 567, 'कोआले': 568, 'कोई': 569, 'कोईसा': 570, 'कोकशोक': 571, 'कोटे': 572, 'कोने': 573, 'कोयले': 574, 'कोरिया': 575, 'कोलम्बस': 576, 'कोशिश': 577, 'कोशीश': 578, 'कोहरा': 579, 'कौआ': 580, 'कौन': 581, 'कौनसा': 582, 'कौनसी': 583, 'कौनसे': 584, 'कौपी': 585, 'कौवे': 586, 'क्या': 587, 'क्यूट': 588, 'क्यों': 589, 'क्योंकि': 590, 'क्योटो': 591, 'क्रिकेट': 592, 'क्लब': 593, 'क्लास': 594, 'क्लासरूम': 595, 'क्वाला': 596, 'क्षमा': 597, 'खटखटाने': 598, 'खटखटाया।': 599, 'खट्टे': 600, 'खड़ा': 601, 'खड़ी': 602, 'खड़े': 603, 'खतम': 604, 'खतरनाक': 605, 'खतरे': 606, 'खबर': 607, 'खम्बे': 608, 'खयाल': 609, 'खरगोश': 610, 'खराब': 611, 'खराबी': 612, 'खरीद': 613, 'खरीदकर': 614, 'खरीदना': 615, 'खरीदने': 616, 'खरीदा।': 617, 'खरीदी': 618, 'खरीदीं।': 619, 'खरीदी।': 620, 'खरीदे।': 621, 'खर्च': 622, 'खर्चना': 623, 'खर्चे': 624, 'ख़ज़ाना': 625, 'ख़तरनाक': 626, 'ख़तरे': 627, 'ख़त्म': 628, 'ख़याल': 629, 'ख़राब': 630, 'ख़रीद': 631, 'ख़रीदने': 632, 'ख़रीदीं': 633, 'ख़रीदीं।': 634, 'ख़रीदो': 635, 'ख़ासियत': 636, 'ख़िलाफ़': 637, 'ख़ुदा': 638, 'ख़ूब': 639, 'खा': 640, 'खाँसी': 641, 'खाए': 642, 'खाएँगे': 643, 'खाकर': 644, 'खाता': 645, 'खाते': 646, 'खाना': 647, 'खानी': 648, 'खाने': 649, 'खाया': 650, 'खाया।': 651, 'खाली': 652, 'खालूँ।': 653, 'खिड़कियाँ': 654, 'खिड़की': 655, 'खिल': 656, 'खिलते': 657, 'खिलाईएगा।': 658, 'खिलाड़ी': 659, 'खिलाना': 660, 'खिलाने': 661, 'खिलाफ़': 662, 'खिलौना': 663, 'खिसक': 664, 'खींचा।': 665, 'खींची।': 666, 'खींज': 667, 'खुद': 668, 'खुल': 669, 'खुलकर': 670, 'खुलता': 671, 'खुला': 672, 'खुली': 673, 'खुश': 674, 'खुशकिसमत': 675, 'खुशखबर': 676, 'खुशी': 677, 'खूबसूरत': 678, 'खेत': 679, 'खेद': 680, 'खेल': 681, 'खेलता': 682, 'खेलती': 683, 'खेलतें': 684, 'खेलना': 685, 'खेलने': 686, 'खो': 687, 'खोद': 688, 'खोदा।': 689, 'खोना': 690, 'खोल': 691, 'खोलते': 692, 'खोलिए।': 693, 'खोलो': 694, 'खोलो।': 695, 'गंदे': 696, 'गई': 697, 'गईं': 698, 'गईं।': 699, 'गई।': 700, 'गए': 701, 'गए।': 702, 'गड्ढा': 703, 'गणित': 704, 'गति': 705, 'गप': 706, 'गबन': 707, 'गया': 708, 'गया।': 709, 'गरम': 710, 'गरीब': 711, 'गर्मियाँ': 712, 'गर्मी': 713, 'गर्व': 714, 'गलत': 715, 'गलतियों': 716, 'गलती': 717, 'गला': 718, 'गवा': 719, 'गवाह': 720, 'गस्सा': 721, 'गहरा': 722, 'गहरी': 723, 'ग़रीब': 724, 'ग़रीबी': 725, 'ग़लत': 726, 'ग़लतफ़ैमी': 727, 'ग़लतियाँ': 728, 'ग़लतियों': 729, 'ग़लती': 730, 'ग़ायब': 731, 'ग़ुलाम': 732, 'ग़ौर': 733, 'गा': 734, 'गाँव': 735, 'गाड़ियाँ': 736, 'गाड़ी': 737, 'गाता': 738, 'गाते': 739, 'गाना': 740, 'गाने': 741, 'गाय': 742, 'गाल': 743, 'गाली': 744, 'गिटार': 745, 'गिनती': 746, 'गिर': 747, 'गिरते': 748, 'गिरनी': 749, 'गिरफ़्तार': 750, 'गिरफ्तार': 751, 'गिरा': 752, 'गिलहरी': 753, 'गिलास': 754, 'गीलीं': 755, 'गुंजाईश': 756, 'गुड़गुड़ा': 757, 'गुड़िया': 758, 'गुफ़ा': 759, 'गुलदान': 760, 'गुलाब': 761, 'गुस्सा': 762, 'गेंद': 763, 'गेट': 764, 'गो': 765, 'गोद': 766, 'गोरे': 767, 'गोल': 768, 'गोलियों': 769, 'गोली': 770, 'गोलीबारी': 771, 'गौल्फ़': 772, 'ग्यारह': 773, 'ग्राहक': 774, 'ग्रीक': 775, 'ग्रुप': 776, 'ग्लास': 777, 'घंटी': 778, 'घंटे': 779, 'घंटो': 780, 'घंटों': 781, 'घटना': 782, 'घड़ी': 783, 'घमण्डी': 784, 'घर': 785, 'घरवालों': 786, 'घायल': 787, 'घास': 788, 'घिनौना': 789, 'घुसते': 790, 'घुसा': 791, 'घुसाना': 792, 'घुसा।': 793, 'घूमती': 794, 'घूमफिरकर': 795, 'घूरकर': 796, 'घूरने': 797, 'घोड़ा': 798, 'घोड़े': 799, 'घोसले': 800, 'चंद': 801, 'चखकर': 802, 'चचेरा': 803, 'चट्टान': 804, 'चढ़': 805, 'चढ़ने': 806, 'चढ़ा।': 807, 'चपटी': 808, 'चमड़े': 809, 'चमत्कार': 810, 'चरने': 811, 'चरित्र': 812, 'चर्च': 813, 'चल': 814, 'चलता': 815, 'चलती': 816, 'चलते': 817, 'चलतें': 818, 'चलना': 819, 'चलने': 820, 'चला': 821, 'चलाकर': 822, 'चलाना': 823, 'चलानी': 824, 'चलाने': 825, 'चली': 826, 'चले': 827, 'चलें': 828, 'चलेगा।': 829, 'चलो': 830, 'चश्मदीद': 831, 'चश्मा': 832, 'चश्मे': 833, 'चहिए': 834, 'चहिए।': 835, 'चाँद': 836, 'चाकू': 837, 'चाचा': 838, 'चादर': 839, 'चादरें': 840, 'चाबियों': 841, 'चाबी': 842, 'चाय': 843, 'चार': 844, 'चाल': 845, 'चालू': 846, 'चावल': 847, 'चाहता': 848, 'चाहता।': 849, 'चाहती': 850, 'चाहते': 851, 'चाहिए': 852, 'चाहिएँ।': 853, 'चाहिए।': 854, 'चाहूँगा।': 855, 'चाहूँगी।': 856, 'चाहे': 857, 'चाहेंगे': 858, 'चाहोगी': 859, 'चाह्ता': 860, 'चाह्ती': 861, 'चाह्ते': 862, 'चिंता': 863, 'चिंतित': 864, 'चिकित्सक': 865, 'चिट्ठियाँ': 866, 'चिट्ठी': 867, 'चिड़िया': 868, 'चिड़ियाँ': 869, 'चिढ़': 870, 'चित': 871, 'चियर्स': 872, 'चिल्ला': 873, 'चिल्लाईए': 874, 'चिल्लाओ': 875, 'चिल्लाने': 876, 'चिल्लाया।': 877, 'चीज़': 878, 'चीज़ें': 879, 'चीन': 880, 'चीनी': 881, 'चीनीवाला': 882, 'चुकताऊँगा।': 883, 'चुका': 884, 'चुकी': 885, 'चुकीं': 886, 'चुके': 887, 'चुकें': 888, 'चुना': 889, 'चुनाव': 890, 'चुनो।': 891, 'चुरा': 892, 'चुराने': 893, 'चूरचूर': 894, 'चूहा': 895, 'चूहे': 896, 'चेतावनी': 897, 'चोट': 898, 'चोटी': 899, 'चोर': 900, 'चोरी': 901, 'चोरों': 902, 'चौंक': 903, 'चौड़ा': 904, 'चौड़ी': 905, 'चौबीस': 906, 'छः': 907, 'छटी': 908, 'छत': 909, 'छपेगी।': 910, 'छलाँग': 911, 'छलांग': 912, 'छा': 913, 'छाँओं': 914, 'छाते': 915, 'छानबीन': 916, 'छाया': 917, 'छिपा': 918, 'छिपाई।': 919, 'छीन': 920, 'छुट्टी': 921, 'छुपी': 922, 'छूट': 923, 'छूटता': 924, 'छूटते': 925, 'छूना': 926, 'छूने': 927, 'छोटा': 928, 'छोटी': 929, 'छोटे': 930, 'छोड़': 931, 'छोड़दी।': 932, 'छोड़ना': 933, 'छोड़नी': 934, 'छोड़ने': 935, 'छोड़ी।': 936, 'छोड़ूँ': 937, 'छोड़े': 938, 'छोड़ो।': 939, 'जंग': 940, 'जंगल': 941, 'जंगली': 942, 'जगह': 943, 'जगे': 944, 'जचती': 945, 'जताई।': 946, 'जनता': 947, 'जनसंख्या': 948, 'जन्म': 949, 'जन्मदिन': 950, 'जब': 951, 'जरूरी': 952, 'जर्मन': 953, 'जर्मनी': 954, 'जल': 955, 'जलकर': 956, 'जलन': 957, 'जलने': 958, 'जलाती': 959, 'जल्द': 960, 'जल्दबाज़ी': 961, 'जल्दसेजल्द': 962, 'जल्दी': 963, 'जवान': 964, 'जवानी': 965, 'जवाब': 966, 'जहाँ': 967, 'जहां': 968, 'जहाज़': 969, 'ज़बरदस्त': 970, 'ज़बान': 971, 'ज़रूर': 972, 'ज़रूरत': 973, 'ज़रूरतमंदों': 974, 'ज़रूरी': 975, 'ज़हरीले': 976, 'ज़िंदगी': 977, 'ज़िंदा': 978, 'ज़िद्दी': 979, 'ज़िन्दगी': 980, 'ज़िन्दा': 981, 'ज़िम्मेदार': 982, 'ज़ुकाम': 983, 'ज़ू': 984, 'ज़ोर': 985, 'ज़्यादा': 986, 'ज़्यादातर': 987, 'जा': 988, 'जाँचपड़ताल': 989, 'जाईये।': 990, 'जाऊँगा।': 991, 'जाऊँगी।': 992, 'जाऊँ।': 993, 'जाए': 994, 'जाएँगे': 995, 'जाएँगे।': 996, 'जाएं': 997, 'जाएं।': 998, 'जाएगा': 999, 'जाएगा।': 1000, 'जाएगी': 1001, 'जाएगी।': 1002, 'जाए।': 1003, 'जाओ': 1004, 'जाओगी।': 1005, 'जाओगे': 1006, 'जाओगे।': 1007, 'जाओ।': 1008, 'जाकर': 1009, 'जागना': 1010, 'जागे': 1011, 'जाचुका': 1012, 'जाता': 1013, 'जाता।': 1014, 'जाती': 1015, 'जातीं': 1016, 'जाते': 1017, 'जातें': 1018, 'जाते।': 1019, 'जान': 1020, 'जानता': 1021, 'जानता।': 1022, 'जानती': 1023, 'जानते': 1024, 'जानना': 1025, 'जानने': 1026, 'जानपहचान': 1027, 'जानबूझकर': 1028, 'जानवर': 1029, 'जानवरों': 1030, 'जाना': 1031, 'जाना।': 1032, 'जाने': 1033, 'जानें': 1034, 'जानेमाने': 1035, 'जाने।': 1036, 'जापान': 1037, 'जापानियों': 1038, 'जापानी': 1039, 'जाम': 1040, 'जायदाद': 1041, 'जाया': 1042, 'जारी': 1043, 'जार्ज': 1044, 'जितना': 1045, 'जितनी': 1046, 'जिन': 1047, 'जिनका': 1048, 'जिमखाने': 1049, 'जिसका': 1050, 'जिसकी': 1051, 'जिसके': 1052, 'जिसने': 1053, 'जिसपर': 1054, 'जिसमें': 1055, 'जिससे': 1056, 'जिसे': 1057, 'जी': 1058, 'जीत': 1059, 'जीतीं': 1060, 'जीतेंगे।': 1061, 'जीतेगा': 1062, 'जीना': 1063, 'जीने': 1064, 'जीन्स': 1065, 'जीव': 1066, 'जी।': 1067, 'जुटाने': 1068, 'जुड़वा': 1069, 'जुर्म': 1070, 'जुर्माना': 1071, 'जुर्रत': 1072, 'जूड़े': 1073, 'जूते': 1074, 'जूतों': 1075, 'जून': 1076, 'जेब': 1077, 'जेल': 1078, 'जैकेट': 1079, 'जैम': 1080, 'जैसा': 1081, 'जैसी': 1082, 'जैसे': 1083, 'जॉन': 1084, 'जो': 1085, 'जोखिम': 1086, 'जोड़ना': 1087, 'जोड़ा।': 1088, 'जोड़ें।': 1089, 'ज्यादा': 1090, 'झगड़ा': 1091, 'झलक': 1092, 'झिलता': 1093, 'झील': 1094, 'झुँको।': 1095, 'झुकी।': 1096, 'झूठ': 1097, 'झूठी': 1098, 'झेला': 1099, 'टर्मिनल': 1100, 'टहलें।': 1101, 'टाँगा': 1102, 'टांगें': 1103, 'टाइम': 1104, 'टाई': 1105, 'टाईम': 1106, 'टिकट': 1107, 'टिकटें': 1108, 'टीचर': 1109, 'टीवी': 1110, 'टीशर्ट': 1111, 'टुकड़ा': 1112, 'टुकड़ो': 1113, 'टुकड़ों': 1114, 'टूट': 1115, 'टूटी': 1116, 'टूटे': 1117, 'टेक्सी': 1118, 'टेनिस': 1119, 'टेरेसा': 1120, 'टेस्ट': 1121, 'टैक्स': 1122, 'टैक्सी': 1123, 'टॉम': 1124, 'टॉयलेट': 1125, 'टोक्यो': 1126, 'टोपी': 1127, 'टोयोटा': 1128, 'ट्राए': 1129, 'ट्राफ़िक': 1130, 'ट्रेन': 1131, 'ट्रेफ़िक': 1132, 'ट्रैफ़िक': 1133, 'ठंड': 1134, 'ठंडा': 1135, 'ठहराया': 1136, 'ठहरिए।': 1137, 'ठहरे।': 1138, 'ठीक': 1139, 'डब्बा': 1140, 'डब्बे': 1141, 'डर': 1142, 'डरता': 1143, 'डरती': 1144, 'डरते': 1145, 'डरना': 1146, 'डरा': 1147, 'डरो।': 1148, 'डाँट': 1149, 'डाँटा।': 1150, 'डाँवाडोल': 1151, 'डाक': 1152, 'डान्स': 1153, 'डाल': 1154, 'डालकर': 1155, 'डालते': 1156, 'डालदो।': 1157, 'डालना': 1158, 'डालने': 1159, 'डाललो।': 1160, 'डाला।': 1161, 'डालियों': 1162, 'डालीं।': 1163, 'डाली।': 1164, 'डालोगे': 1165, 'डिक्शनरी': 1166, 'डिनर': 1167, 'डुबकी': 1168, 'डूबते': 1169, 'डूबने': 1170, 'डॉक्टर': 1171, 'डॉलर': 1172, 'ड्राईव': 1173, 'ड्रेस': 1174, 'ढंग': 1175, 'ढक': 1176, 'ढल': 1177, 'ढूँढ': 1178, 'ढूँढता': 1179, 'ढूँढना': 1180, 'ढूँढने': 1181, 'ढूँढो।': 1182, 'ढेर': 1183, 'तक': 1184, 'तकरीबन': 1185, 'तकलीफ': 1186, 'तट': 1187, 'तत्पर': 1188, 'तब': 1189, 'तबाह': 1190, 'तभी': 1191, 'तम्बाकू': 1192, 'तम्हे': 1193, 'तम्हें': 1194, 'तरतर': 1195, 'तरफ़': 1196, 'तरह': 1197, 'तरीके': 1198, 'तलाश': 1199, 'तस्कर': 1200, 'तस्वीर': 1201, 'तस्वीरें': 1202, 'तस्वीरों': 1203, 'तहकीकात': 1204, 'ताईवान': 1205, 'ताऊजी': 1206, 'ताकत': 1207, 'ताकतवर': 1208, 'ताकतशाली': 1209, 'ताक़त': 1210, 'ताज': 1211, 'ताज़ा': 1212, 'ताज़ी': 1213, 'तापमान': 1214, 'ताला': 1215, 'तालाब': 1216, 'तीन': 1217, 'तीस': 1218, 'तुम': 1219, 'तुमको': 1220, 'तुमने': 1221, 'तुमपर': 1222, 'तुममें': 1223, 'तुमसे': 1224, 'तुम्हारा': 1225, 'तुम्हारी': 1226, 'तुम्हारे': 1227, 'तुम्हे': 1228, 'तुम्हें': 1229, 'तुरंत': 1230, 'तुरन्त': 1231, 'तू': 1232, 'तूफ़ान': 1233, 'तेज़': 1234, 'तेज़ी': 1235, 'तेरा': 1236, 'तेरी': 1237, 'तेल': 1238, 'तैयार': 1239, 'तैयारी': 1240, 'तैर': 1241, 'तैरकर': 1242, 'तैरना': 1243, 'तैवान': 1244, 'तो': 1245, 'तोड़': 1246, 'तोड़ी': 1247, 'तोड़ोगे': 1248, 'तोड़ो।': 1249, 'तोदाइजी': 1250, 'तोला।': 1251, 'तोहफ़ा': 1252, 'तोहफ़े': 1253, 'तोह्फ़ा': 1254, 'तो।': 1255, 'तौर': 1256, 'तौलिए': 1257, 'तौलिया': 1258, 'तौलिये': 1259, 'त्याग': 1260, 'थक': 1261, 'थकाहारा': 1262, 'थकेहारे': 1263, 'थप्पड़': 1264, 'था': 1265, 'था।': 1266, 'थी': 1267, 'थीं': 1268, 'थीं।': 1269, 'थी।': 1270, 'थूकेगा': 1271, 'थे': 1272, 'थें': 1273, 'थें।': 1274, 'थेम्स': 1275, 'थे।': 1276, 'थोड़ा': 1277, 'थोड़ी': 1278, 'थोड़े': 1279, 'थोड़ेबहुत': 1280, 'दंत': 1281, 'दक्षिण': 1282, 'दफ़तर': 1283, 'दफ़नाया': 1284, 'दफ़्तर': 1285, 'दबाकर': 1286, 'दया': 1287, 'दयालु': 1288, 'दरअसल': 1289, 'दरवाज़ा': 1290, 'दरवाज़े': 1291, 'दराज़ें': 1292, 'दर्जन': 1293, 'दर्द': 1294, 'दल': 1295, 'दवा': 1296, 'दवाई': 1297, 'दवाईयों': 1298, 'दस': 1299, 'दसियों': 1300, 'दहशत': 1301, 'दाँतों': 1302, 'दाढ़ी': 1303, 'दादी': 1304, 'दाहिना': 1305, 'दिए': 1306, 'दिए।': 1307, 'दिक्कत': 1308, 'दिख': 1309, 'दिखता': 1310, 'दिखते': 1311, 'दिखने': 1312, 'दिखाई': 1313, 'दिखाईए।': 1314, 'दिखाई।': 1315, 'दिखाऊँगा': 1316, 'दिखाओ।': 1317, 'दिखाना': 1318, 'दिखाया।': 1319, 'दिन': 1320, 'दिनों': 1321, 'दिमाग़': 1322, 'दिया': 1323, 'दिया।': 1324, 'दियें': 1325, 'दिल': 1326, 'दिलचस्प': 1327, 'दिलचस्पी': 1328, 'दिलाती': 1329, 'दिलाया।': 1330, 'दिल्ली': 1331, 'दिवस': 1332, 'दिवाई': 1333, 'दिवालिया': 1334, 'दिशा': 1335, 'दिशाओं': 1336, 'दी': 1337, 'दीं': 1338, 'दीं।': 1339, 'दीजिएगा।': 1340, 'दीजिए।': 1341, 'दीदी': 1342, 'दीवार': 1343, 'दीवालिया': 1344, 'दी।': 1345, 'दुःखी': 1346, 'दुकान': 1347, 'दुख': 1348, 'दुखी': 1349, 'दुनिया': 1350, 'दुरुपयोग': 1351, 'दुर्घटना': 1352, 'दुल्हन': 1353, 'दुश्मन': 1354, 'दुश्मनों': 1355, 'दूँगा।': 1356, 'दूंगा।': 1357, 'दूध': 1358, 'दूर': 1359, 'दूरदूर': 1360, 'दूरी': 1361, 'दूसरा': 1362, 'दूसरे': 1363, 'दूसरों': 1364, 'दे': 1365, 'देंगे': 1366, 'देंगे।': 1367, 'देख': 1368, 'देखकर': 1369, 'देखता': 1370, 'देखता।': 1371, 'देखते': 1372, 'देखतें': 1373, 'देखना': 1374, 'देखनी': 1375, 'देखने': 1376, 'देखभाल': 1377, 'देखा': 1378, 'देखा।': 1379, 'देखी': 1380, 'देखे': 1381, 'देखो': 1382, 'देगी।': 1383, 'देता': 1384, 'देतीं': 1385, 'देते': 1386, 'देदीजिए।': 1387, 'देना': 1388, 'देना।': 1389, 'देनी': 1390, 'देने': 1391, 'देर': 1392, 'देश': 1393, 'देशद्रोही': 1394, 'देशों': 1395, 'दो': 1396, 'दोगे': 1397, 'दोगे।': 1398, 'दोनो': 1399, 'दोनों': 1400, 'दोपहर': 1401, 'दोपहरभर': 1402, 'दोबारा': 1403, 'दोस्त': 1404, 'दोस्ती': 1405, 'दोस्तों': 1406, 'दो।': 1407, 'दौड़': 1408, 'दौड़ते': 1409, 'दौड़ने': 1410, 'दौरान': 1411, 'द्वार': 1412, 'द्वीप': 1413, 'धन्यवाद।': 1414, 'धरती': 1415, 'धर्म': 1416, 'धातु': 1417, 'धीमी': 1418, 'धीरे': 1419, 'धीरेधीरे': 1420, 'धुंध': 1421, 'धुलवानीं': 1422, 'धुलाई': 1423, 'धूम्रपान': 1424, 'धूल': 1425, 'धोओ।': 1426, 'धोका': 1427, 'धोखा': 1428, 'धोने': 1429, 'ध्यान': 1430, 'ध्यानपूर्वक': 1431, 'न': 1432, 'नई': 1433, 'नए': 1434, 'नकल': 1435, 'नकली': 1436, 'नक्शा': 1437, 'नगर': 1438, 'नगरों': 1439, 'नज़र': 1440, 'नटखट': 1441, 'नतीजे': 1442, 'नदी': 1443, 'नन': 1444, 'नन्हा': 1445, 'नफ़रत': 1446, 'नब्ज़': 1447, 'नमक': 1448, 'नमस्कार।': 1449, 'नमस्ते।': 1450, 'नम्बर': 1451, 'नया': 1452, 'नर्म': 1453, 'नव': 1454, 'नष्ट': 1455, 'नसीब': 1456, 'नसीबवाला': 1457, 'नहा': 1458, 'नहाता': 1459, 'नहाने': 1460, 'नहीँ': 1461, 'नहीं': 1462, 'नहीं।': 1463, 'ना': 1464, 'नाई': 1465, 'नाकामयाब': 1466, 'नाकामयाबी': 1467, 'नागरिक': 1468, 'नाचना': 1469, 'नाज़': 1470, 'नाना': 1471, 'नानी': 1472, 'नाप': 1473, 'नाम': 1474, 'नामुमकिन': 1475, 'नारंगी': 1476, 'नाराज़': 1477, 'नाव': 1478, 'नाश्ता': 1479, 'नाश्ते': 1480, 'निकम्मा': 1481, 'निकल': 1482, 'निकलने': 1483, 'निकला': 1484, 'निकली।': 1485, 'निकले।': 1486, 'निकलो': 1487, 'निकाल': 1488, 'निकालते': 1489, 'निकालने': 1490, 'निकाला': 1491, 'निकाला।': 1492, 'निगल': 1493, 'निधन': 1494, 'निभता': 1495, 'निभाएगा।': 1496, 'निभाओ।': 1497, 'निभाना': 1498, 'नियम': 1499, 'नियमों': 1500, 'निराश': 1501, 'निराशा': 1502, 'निर्णय': 1503, 'निर्दोष': 1504, 'निर्भर': 1505, 'निर्यात': 1506, 'निशान': 1507, 'निशाना': 1508, 'निशानों': 1509, 'निश्चित': 1510, 'नींद': 1511, 'नीचे': 1512, 'नीति': 1513, 'नीला': 1514, 'नीली': 1515, 'नीले': 1516, 'ने': 1517, 'नें': 1518, 'नौ': 1519, 'नौकर': 1520, 'नौकरानी': 1521, 'नौकरी': 1522, 'नौवे': 1523, 'न्यू': 1524, 'न्यूयॉर्क': 1525, 'न्योता': 1526, 'पंख': 1527, 'पंछी': 1528, 'पंद्रह': 1529, 'पंद्राह': 1530, 'पकड़': 1531, 'पकड़कर': 1532, 'पकड़नी': 1533, 'पकड़ने': 1534, 'पकड़ा': 1535, 'पकड़ी': 1536, 'पकड़ीं': 1537, 'पका': 1538, 'पकाया': 1539, 'पकाया।': 1540, 'पक्का': 1541, 'पक्ष': 1542, 'पक्षी': 1543, 'पचास': 1544, 'पछतावा': 1545, 'पट': 1546, 'पटरी': 1547, 'पड़': 1548, 'पड़ता': 1549, 'पड़ता।': 1550, 'पड़ती।': 1551, 'पड़ा': 1552, 'पड़ा।': 1553, 'पड़ी': 1554, 'पड़ीं।': 1555, 'पड़ी।': 1556, 'पड़े': 1557, 'पड़ेगा।': 1558, 'पड़ेगी।': 1559, 'पड़ोस': 1560, 'पड़ोसियों': 1561, 'पड़ोसी': 1562, 'पढ़': 1563, 'पढ़के': 1564, 'पढ़ता': 1565, 'पढ़ते': 1566, 'पढ़ना': 1567, 'पढ़ने': 1568, 'पढ़लो।': 1569, 'पढ़ा': 1570, 'पढ़ाई': 1571, 'पढ़ाना': 1572, 'पढ़ी': 1573, 'पढ़ूँ।': 1574, 'पढ़ें।': 1575, 'पढ़ो': 1576, 'पढ़ो।': 1577, 'पतंग': 1578, 'पतला': 1579, 'पतली': 1580, 'पता': 1581, 'पता।': 1582, 'पति': 1583, 'पत्तियाँ': 1584, 'पत्तियों': 1585, 'पत्थर': 1586, 'पत्नी': 1587, 'पत्र': 1588, 'पनीर': 1589, 'पर': 1590, 'परसों': 1591, 'परिक्रमा': 1592, 'परिचित': 1593, 'परियों': 1594, 'परिवार': 1595, 'परिस्तिथि': 1596, 'परिस्थिति': 1597, 'परीक्षा': 1598, 'परेशान': 1599, 'परेशानी': 1600, 'पर्यावरण': 1601, 'पर्वत': 1602, 'पर्स': 1603, 'पर।': 1604, 'पल्ले': 1605, 'पश्चिम': 1606, 'पश्चिमी': 1607, 'पसंद': 1608, 'पसंदीता': 1609, 'पसीना': 1610, 'पहचान': 1611, 'पहचानता': 1612, 'पहचानते': 1613, 'पहचाना': 1614, 'पहनती': 1615, 'पहनी': 1616, 'पहनूँ।': 1617, 'पहने': 1618, 'पहनोगी': 1619, 'पहरा': 1620, 'पहला': 1621, 'पहली': 1622, 'पहलू': 1623, 'पहले': 1624, 'पहाड़': 1625, 'पहुँच': 1626, 'पहुँचते': 1627, 'पहुँचने': 1628, 'पहुँचा': 1629, 'पहुँचाई।': 1630, 'पहुँचाएगा।': 1631, 'पहुँचाने': 1632, 'पहुँचे': 1633, 'पहुँचें': 1634, 'पहुँचेगा।': 1635, 'पहुँचे।': 1636, 'पहुंच': 1637, 'पा': 1638, 'पाँच': 1639, 'पाँचगुना': 1640, 'पाँचवी': 1641, 'पांव': 1642, 'पाइप': 1643, 'पाई।': 1644, 'पाऊँगा।': 1645, 'पाएगा।': 1646, 'पाए।': 1647, 'पाओ।': 1648, 'पागल': 1649, 'पाता': 1650, 'पाते।': 1651, 'पानी': 1652, 'पाने': 1653, 'पापा': 1654, 'पापामम्मी': 1655, 'पाय': 1656, 'पाया': 1657, 'पाया।': 1658, 'पायी।': 1659, 'पायेंगे।': 1660, 'पार': 1661, 'पारित': 1662, 'पार्क': 1663, 'पार्टी': 1664, 'पार्सल': 1665, 'पालतू': 1666, 'पालन': 1667, 'पालनी': 1668, 'पालने': 1669, 'पालपोस': 1670, 'पास': 1671, 'पासपोर्ट': 1672, 'पिंक': 1673, 'पिंजरा': 1674, 'पिछली': 1675, 'पिछले': 1676, 'पिता': 1677, 'पिताजी': 1678, 'पिया': 1679, 'पियानिस्ट': 1680, 'पियानो': 1681, 'पियो।': 1682, 'पी': 1683, 'पीकर': 1684, 'पीछा': 1685, 'पीछे': 1686, 'पीछेपीछे': 1687, 'पीठ': 1688, 'पीते': 1689, 'पीना': 1690, 'पीनी': 1691, 'पीने': 1692, 'पी।': 1693, 'पुकारते': 1694, 'पुरस्कार': 1695, 'पुराना': 1696, 'पुरानी': 1697, 'पुराने': 1698, 'पुलिस': 1699, 'पुलिसवाला': 1700, 'पुलीस': 1701, 'पुलीसवाले': 1702, 'पुस्तकालय': 1703, 'पूंछ': 1704, 'पूछ': 1705, 'पूछते': 1706, 'पूछना': 1707, 'पूछने': 1708, 'पूछा': 1709, 'पूछूँगा।': 1710, 'पूज्य': 1711, 'पूरा': 1712, 'पूरी': 1713, 'पूरे': 1714, 'पृथ्वी': 1715, 'पृष्ठ': 1716, 'पे': 1717, 'पेंट': 1718, 'पेट': 1719, 'पेट्रोल': 1720, 'पेड़': 1721, 'पेड़पौधों': 1722, 'पेनसिल': 1723, 'पेनसिलें': 1724, 'पेशा': 1725, 'पैंट': 1726, 'पैक': 1727, 'पैदा': 1728, 'पैमाने': 1729, 'पैर': 1730, 'पैरिस': 1731, 'पैरों': 1732, 'पैसा': 1733, 'पैसे': 1734, 'पैसेवाला': 1735, 'पैसेवाली': 1736, 'पैसेवाले': 1737, 'पैसों': 1738, 'पोंछा।': 1739, 'पोंछो।': 1740, 'पोत': 1741, 'पोती': 1742, 'पौधे': 1743, 'प्यार': 1744, 'प्यास': 1745, 'प्रकट': 1746, 'प्रतियोगिता': 1747, 'प्रतिशत': 1748, 'प्रतीक्षा': 1749, 'प्रथम': 1750, 'प्रदूषण': 1751, 'प्रदूषित': 1752, 'प्रभाव': 1753, 'प्रयत्न': 1754, 'प्रयास': 1755, 'प्रयोगों': 1756, 'प्रश्न': 1757, 'प्रसन्न': 1758, 'प्रसिद्ध': 1759, 'प्रस्ताव': 1760, 'प्राप्त': 1761, 'प्रेम': 1762, 'प्रेमपूर्वक': 1763, 'प्रेरणा': 1764, 'प्रेसीडेंट': 1765, 'प्रॉडक्ट्स': 1766, 'प्लान': 1767, 'प्लीज़।': 1768, 'प्लैन': 1769, 'फँस': 1770, 'फंस': 1771, 'फट': 1772, 'फटाफट': 1773, 'फ़रिश्तों': 1774, 'फ़र्क': 1775, 'फ़र्निचर': 1776, 'फ़ायदा': 1777, 'फ़िनलैंड': 1778, 'फ़िल्म': 1779, 'फ़िल्में': 1780, 'फ़ुट': 1781, 'फ़ुटबॉल': 1782, 'फ़ेसबुक': 1783, 'फ़ैक्टरी': 1784, 'फ़ैसला': 1785, 'फ़ोटोकॉपियर': 1786, 'फ़ोन': 1787, 'फ़ोल्डर': 1788, 'फ़ौरन': 1789, 'फ़्रांस': 1790, 'फ़्रांसीसी': 1791, 'फ़्रानसीसी': 1792, 'फ़्रान्स': 1793, 'फ़्रेंच': 1794, 'फ़्लैट': 1795, 'फाड़': 1796, 'फिर': 1797, 'फिरसे': 1798, 'फूल': 1799, 'फूलों': 1800, 'फेंक': 1801, 'फेंकिए।': 1802, 'फेंको।': 1803, 'फैलती': 1804, 'फैला': 1805, 'फैलाव': 1806, 'फैले': 1807, 'फ्रांस': 1808, 'फ्रेंच': 1809, 'बंद': 1810, 'बंदर': 1811, 'बंदूक': 1812, 'बकबक': 1813, 'बकरियों': 1814, 'बकवास': 1815, 'बगीचा': 1816, 'बगीचे': 1817, 'बगैर': 1818, 'बचा': 1819, 'बचाए।': 1820, 'बचाओ': 1821, 'बचाया।': 1822, 'बचाली।': 1823, 'बचीं': 1824, 'बचे': 1825, 'बच्चा': 1826, 'बच्ची': 1827, 'बच्चे': 1828, 'बच्चें': 1829, 'बच्चों': 1830, 'बज': 1831, 'बजने': 1832, 'बजा': 1833, 'बजाए': 1834, 'बजाता': 1835, 'बजाना': 1836, 'बजाय': 1837, 'बजा।': 1838, 'बजे': 1839, 'बटुए': 1840, 'बड़ा': 1841, 'बड़ाई': 1842, 'बड़ी': 1843, 'बड़े': 1844, 'बढ़': 1845, 'बढ़ाचढ़ाकर': 1846, 'बढ़ाने': 1847, 'बढ़ावा': 1848, 'बढ़िया': 1849, 'बढ़ी।': 1850, 'बता': 1851, 'बताईये।': 1852, 'बताए': 1853, 'बताओ': 1854, 'बताओ।': 1855, 'बतादूँगा।': 1856, 'बतादेना।': 1857, 'बताने': 1858, 'बताया': 1859, 'बताया।': 1860, 'बत्तियाँ': 1861, 'बत्ती': 1862, 'बदतमीज़': 1863, 'बदतर': 1864, 'बदबू': 1865, 'बदल': 1866, 'बदलने': 1867, 'बदला': 1868, 'बदले': 1869, 'बधाई': 1870, 'बधाईंयाँ।': 1871, 'बधाईयाँ।': 1872, 'बन': 1873, 'बनता': 1874, 'बनते': 1875, 'बनना': 1876, 'बनने': 1877, 'बनवानी': 1878, 'बनवालिया।': 1879, 'बना': 1880, 'बनाई': 1881, 'बनाए': 1882, 'बनाओगे': 1883, 'बनाओ।': 1884, 'बनातीं': 1885, 'बनातें': 1886, 'बनाना': 1887, 'बनाने': 1888, 'बनाया': 1889, 'बनी': 1890, 'बनो।': 1891, 'बमबई': 1892, 'बयान': 1893, 'बरफ़': 1894, 'बरबाद': 1895, 'बरस': 1896, 'बरसात': 1897, 'बरी': 1898, 'बर्तन': 1899, 'बर्ताव': 1900, 'बर्दाश्त': 1901, 'बर्न': 1902, 'बर्फ़': 1903, 'बर्बादी': 1904, 'बल्कि': 1905, 'बस': 1906, 'बसीं।': 1907, 'बसे': 1908, 'बसे।': 1909, 'बस्ता': 1910, 'बस्ता।': 1911, 'बहतर': 1912, 'बहन': 1913, 'बहनें': 1914, 'बहस': 1915, 'बहादुर': 1916, 'बहादुरी': 1917, 'बहाना': 1918, 'बहार': 1919, 'बहुत': 1920, 'बाँट': 1921, 'बाँटा।': 1922, 'बाँधती': 1923, 'बांध': 1924, 'बाएं': 1925, 'बाकी': 1926, 'बाग': 1927, 'बाघ': 1928, 'बाज़ार': 1929, 'बात': 1930, 'बातचीत': 1931, 'बाताऊँगा।': 1932, 'बाताऊँगी।': 1933, 'बातें': 1934, 'बाथरूम': 1935, 'बाद': 1936, 'बादल': 1937, 'बाप': 1938, 'बार': 1939, 'बारह': 1940, 'बारिश': 1941, 'बारी': 1942, 'बारे': 1943, 'बार।': 1944, 'बाल': 1945, 'बालों': 1946, 'बावर्ची': 1947, 'बास्\\u200dकेटबॉल': 1948, 'बाहर': 1949, 'बिक': 1950, 'बिखेर': 1951, 'बिगड़े': 1952, 'बिछाई।': 1953, 'बिछी': 1954, 'बिजली': 1955, 'बिज़ी': 1956, 'बिताई।': 1957, 'बिना': 1958, 'बिल': 1959, 'बिलकुल': 1960, 'बिल्लियाँ': 1961, 'बिल्लियों': 1962, 'बिल्ली': 1963, 'बिस्टर': 1964, 'बिस्तर': 1965, 'बीच': 1966, 'बीजों': 1967, 'बीटल्स': 1968, 'बीतता': 1969, 'बीमार': 1970, 'बीमारी': 1971, 'बीमारों': 1972, 'बीयर': 1973, 'बीवा': 1974, 'बीवी': 1975, 'बीस': 1976, 'बुआजी': 1977, 'बुखार': 1978, 'बुरा': 1979, 'बुराई': 1980, 'बुरी': 1981, 'बुरे': 1982, 'बुलवाई।': 1983, 'बुलाई।': 1984, 'बुलाएँगे।': 1985, 'बुलाना': 1986, 'बुलाया': 1987, 'बुलाया।': 1988, 'बूंद': 1989, 'बूढ़ा': 1990, 'बूढ़ी': 1991, 'बूढ़े': 1992, 'बॅटर': 1993, 'बेकरी': 1994, 'बेकार': 1995, 'बेगुनाह': 1996, 'बेच': 1997, 'बेचती': 1998, 'बेची': 1999, 'बेचैन': 2000, 'बेटा': 2001, 'बेटियाँ': 2002, 'बेटी': 2003, 'बेटे': 2004, 'बेबुनयाद': 2005, 'बेसबॉल': 2006, 'बेसब्री': 2007, 'बेहतर': 2008, 'बेहोश': 2009, 'बेहोशी': 2010, 'बैंक': 2011, 'बैक': 2012, 'बैट': 2013, 'बैठ': 2014, 'बैठते': 2015, 'बैठना': 2016, 'बैठा': 2017, 'बैठा।': 2018, 'बैठिए।': 2019, 'बैठिये।': 2020, 'बैठी': 2021, 'बैठो।': 2022, 'बॉल': 2023, 'बॉस': 2024, 'बॉस्टन': 2025, 'बोओगे': 2026, 'बोतलें': 2027, 'बोर': 2028, 'बोल': 2029, 'बोलता': 2030, 'बोलती': 2031, 'बोलतीं': 2032, 'बोलते': 2033, 'बोलना': 2034, 'बोलनी': 2035, 'बोलने': 2036, 'बोला': 2037, 'बोला।': 2038, 'बोलिए।': 2039, 'बोली': 2040, 'बोलूँ': 2041, 'बोलो': 2042, 'बोलो।': 2043, 'बौद्ध': 2044, 'ब्रश': 2045, 'ब्राऊन': 2046, 'ब्रिज': 2047, 'ब्रिटेन': 2048, 'ब्रॅड': 2049, 'ब्रेड': 2050, 'ब्लडी': 2051, 'भगवान': 2052, 'भयानक': 2053, 'भर': 2054, 'भरना': 2055, 'भरना।': 2056, 'भरनी।': 2057, 'भरी।': 2058, 'भरेगा।': 2059, 'भरोसा': 2060, 'भला': 2061, 'भला।': 2062, 'भविष्य': 2063, 'भाई': 2064, 'भाईबहन': 2065, 'भाग': 2066, 'भागता': 2067, 'भागते': 2068, 'भागा।': 2069, 'भागे': 2070, 'भाग्य': 2071, 'भारत': 2072, 'भारतीय': 2073, 'भारी': 2074, 'भावनाओं': 2075, 'भाषण': 2076, 'भाषा': 2077, 'भाषाएँ': 2078, 'भाषाएं': 2079, 'भाषाओं': 2080, 'भिजवादूँगा।': 2081, 'भिड़': 2082, 'भी': 2083, 'भीग': 2084, 'भुजाना': 2085, 'भुला': 2086, 'भूकम्प': 2087, 'भूख': 2088, 'भूत': 2089, 'भूतप्रेत': 2090, 'भूल': 2091, 'भूलती': 2092, 'भूलना': 2093, 'भूलना।': 2094, 'भूलूँगा।': 2095, 'भेज': 2096, 'भेजा': 2097, 'भेजी': 2098, 'भेजो।': 2099, 'भोजन': 2100, 'भौंकता': 2101, 'भौतिक': 2102, 'भ्रष्टाचार': 2103, 'मंगलवार': 2104, 'मंगवाया': 2105, 'मंज़िल': 2106, 'मंजिल': 2107, 'मंदिर': 2108, 'मंदिरों': 2109, 'मंदी': 2110, 'मई': 2111, 'मकान': 2112, 'मक्खन': 2113, 'मगर': 2114, 'मचाओ।': 2115, 'मछलियाँ': 2116, 'मछली': 2117, 'मछवारे': 2118, 'मजबूर': 2119, 'मज़बूत': 2120, 'मज़ा': 2121, 'मज़ाक': 2122, 'मज़े': 2123, 'मज़ेदार': 2124, 'मजेदार': 2125, 'मत': 2126, 'मतलब': 2127, 'मतलबी': 2128, 'मत।': 2129, 'मदद': 2130, 'मदर': 2131, 'मन': 2132, 'मना': 2133, 'मनाने': 2134, 'मनाया।': 2135, 'मनुष्य': 2136, 'ममेरा': 2137, 'मम्मी': 2138, 'मम्मीपापा': 2139, 'मर': 2140, 'मरना': 2141, 'मरने': 2142, 'मरा।': 2143, 'मरीज़': 2144, 'मर्ज़ी': 2145, 'मर्यादा': 2146, 'मलेरिया': 2147, 'मशहूर': 2148, 'मशीन': 2149, 'मश्हूर': 2150, 'महंगा': 2151, 'महंगी': 2152, 'महक': 2153, 'महल': 2154, 'महसूस': 2155, 'महाद्वीप': 2156, 'महान': 2157, 'महाराष्ट्र': 2158, 'महाशय': 2159, 'महिलाये': 2160, 'महीने': 2161, 'महीनों': 2162, 'माँ': 2163, 'माँगनी': 2164, 'माँगने': 2165, 'माँगा।': 2166, 'माँगे।': 2167, 'माँगो।': 2168, 'माँबाप': 2169, 'मांग': 2170, 'मांगी।': 2171, 'माईने': 2172, 'मातापिता': 2173, 'मात्रा': 2174, 'मात्सुमोतो': 2175, 'माथे': 2176, 'मान': 2177, 'मानता': 2178, 'मानते': 2179, 'मानो': 2180, 'माफ़': 2181, 'माफ़ी': 2182, 'मामले': 2183, 'मामा': 2184, 'मारने': 2185, 'मारा': 2186, 'मारा।': 2187, 'मारी।': 2188, 'मालिक': 2189, 'मालूम': 2190, 'मालूम।': 2191, 'मिनट': 2192, 'मिनटों': 2193, 'मिल': 2194, 'मिलकर': 2195, 'मिलती': 2196, 'मिलतीजुलतीं': 2197, 'मिलते': 2198, 'मिलना': 2199, 'मिलना।': 2200, 'मिलने': 2201, 'मिला': 2202, 'मिलाना': 2203, 'मिला।': 2204, 'मिली': 2205, 'मिलीं': 2206, 'मिली।': 2207, 'मिले': 2208, 'मिलेंगीं।': 2209, 'मिलेंगे।': 2210, 'मिलेगा।': 2211, 'मिलेगी।': 2212, 'मिले।': 2213, 'मीटिंग': 2214, 'मीनार': 2215, 'मील': 2216, 'मुकाबले': 2217, 'मुक्त': 2218, 'मुझको': 2219, 'मुझपर': 2220, 'मुझसे': 2221, 'मुझा': 2222, 'मुझे': 2223, 'मुट्ठीभर': 2224, 'मुड़कर': 2225, 'मुड़ो': 2226, 'मुताबिक': 2227, 'मुनाफ़े': 2228, 'मुफ़्त': 2229, 'मुबारक': 2230, 'मुम्बई': 2231, 'मुरझा': 2232, 'मुर्गियाँ': 2233, 'मुर्गियों': 2234, 'मुर्झाई': 2235, 'मुलाक़ात': 2236, 'मुलाकात': 2237, 'मुश्किल': 2238, 'मुसलाधार': 2239, 'मुसीबत': 2240, 'मुस्कुराई।': 2241, 'मुस्कुराया।': 2242, 'मुस्कुराहट': 2243, 'मूँगफलियाँ': 2244, 'मूँह': 2245, 'मूर्ख': 2246, 'मे': 2247, 'में': 2248, 'मेक्सिको': 2249, 'मेज़': 2250, 'मेट्रो': 2251, 'मेने': 2252, 'मेरा': 2253, 'मेरी': 2254, 'मेरीं': 2255, 'मेरे': 2256, 'मेहनत': 2257, 'मै': 2258, 'मैं': 2259, 'मैंने': 2260, 'मैं।': 2261, 'मैनेजर': 2262, 'मैरी': 2263, 'मोज़े': 2264, 'मोटरसाईकल': 2265, 'मोटा': 2266, 'मोटी': 2267, 'मोटे': 2268, 'मोड़ो।': 2269, 'मोर': 2270, 'मोहन': 2271, 'मौका': 2272, 'मौके': 2273, 'मौज': 2274, 'मौत': 2275, 'मौसम': 2276, 'म्युज़ियम': 2277, 'यकीन': 2278, 'यह': 2279, 'यहाँ': 2280, 'यहाँवहाँ': 2281, 'यहां': 2282, 'यही': 2283, 'यहीं': 2284, 'या': 2285, 'यात्रा': 2286, 'यात्रियों': 2287, 'याद': 2288, 'याददाश्त': 2289, 'युक्तियुक्त': 2290, 'युद्ध': 2291, 'युनिसायकल': 2292, 'युरोप': 2293, 'यूरोप': 2294, 'ये': 2295, 'येन': 2296, 'यॉर्क': 2297, 'योग्यता': 2298, 'योजना': 2299, 'रंग': 2300, 'रंगबिरंगी': 2301, 'रक्षा': 2302, 'रख': 2303, 'रखकर': 2304, 'रखता': 2305, 'रखती': 2306, 'रखदो।': 2307, 'रखना': 2308, 'रखना।': 2309, 'रखनी': 2310, 'रखा': 2311, 'रखा।': 2312, 'रखे': 2313, 'रखो': 2314, 'रखो।': 2315, 'रटना': 2316, 'रफ़तार': 2317, 'रबड़': 2318, 'रवाना': 2319, 'रविवार': 2320, 'रसोइया': 2321, 'रस्सी': 2322, 'रह': 2323, 'रहता': 2324, 'रहता।': 2325, 'रहती': 2326, 'रहतीं': 2327, 'रहते': 2328, 'रहना': 2329, 'रहने': 2330, 'रहस्य': 2331, 'रहा': 2332, 'रहा।': 2333, 'रहिए।': 2334, 'रही': 2335, 'रहीं': 2336, 'रही।': 2337, 'रहूँगा।': 2338, 'रहे': 2339, 'रहें': 2340, 'रहेंगे': 2341, 'रहेंगे।': 2342, 'रहेगा।': 2343, 'रहे।': 2344, 'रहो': 2345, 'रहोगे': 2346, 'रहो।': 2347, 'राख': 2348, 'राजधानी': 2349, 'राजनीति': 2350, 'राज़': 2351, 'राज़ी': 2352, 'राजा': 2353, 'राज्य': 2354, 'रात': 2355, 'रातभर': 2356, 'राय': 2357, 'राष्ट्रपति': 2358, 'राष्ट्रीयता': 2359, 'रास्ता': 2360, 'रास्ते': 2361, 'रिज़ल्ट': 2362, 'रिपोर्ट': 2363, 'रिमोट': 2364, 'रिहा': 2365, 'रुक': 2366, 'रुकी।': 2367, 'रूप': 2368, 'रूसी': 2369, 'रेखांकित': 2370, 'रेडियो': 2371, 'रेत': 2372, 'रेलगाड़ी': 2373, 'रेशम': 2374, 'रेशमी': 2375, 'रेस': 2376, 'रो': 2377, 'रोई।': 2378, 'रोक': 2379, 'रोकना': 2380, 'रोकने': 2381, 'रोका।': 2382, 'रोगी': 2383, 'रोज़': 2384, 'रोजाने': 2385, 'रोडिका': 2386, 'रोते': 2387, 'रोना': 2388, 'रोने': 2389, 'रोम': 2390, 'रोमांचक': 2391, 'रोया': 2392, 'रोया।': 2393, 'लंदन': 2394, 'लकड़ी': 2395, 'लग': 2396, 'लगकर': 2397, 'लगता': 2398, 'लगता।': 2399, 'लगती': 2400, 'लगतीं': 2401, 'लगते': 2402, 'लगना': 2403, 'लगभग': 2404, 'लगा': 2405, 'लगाई।': 2406, 'लगाऊँगा': 2407, 'लगाए': 2408, 'लगाकर': 2409, 'लगातार': 2410, 'लगादी।': 2411, 'लगाया': 2412, 'लगाया।': 2413, 'लगा।': 2414, 'लगी': 2415, 'लगीं': 2416, 'लगी।': 2417, 'लगे': 2418, 'लगेंगे।': 2419, 'लगेगी': 2420, 'लगेगी।': 2421, 'लचीली': 2422, 'लड़का': 2423, 'लड़की': 2424, 'लड़के': 2425, 'लड़नेझगड़ने': 2426, 'लड़ाई': 2427, 'लतपत': 2428, 'लन्च': 2429, 'लम्बा': 2430, 'लम्बी': 2431, 'लम्बीं': 2432, 'लम्बे': 2433, 'ला': 2434, 'लाईं।': 2435, 'लाईन': 2436, 'लाईसेंस': 2437, 'लाई।': 2438, 'लाऊँगा।': 2439, 'लागु': 2440, 'लाता': 2441, 'लाती': 2442, 'लाना।': 2443, 'लापरवाही': 2444, 'लायक': 2445, 'लाया': 2446, 'लाल': 2447, 'लिंकन': 2448, 'लिए': 2449, 'लिए।': 2450, 'लिख': 2451, 'लिखकर': 2452, 'लिखना': 2453, 'लिखा': 2454, 'लिखी': 2455, 'लिखेगा': 2456, 'लिपटा': 2457, 'लिपटी': 2458, 'लिफ़ाफ़े': 2459, 'लिफ़्ट': 2460, 'लिया': 2461, 'लिया।': 2462, 'लिहाज': 2463, 'ली': 2464, 'लीं': 2465, 'लीं।': 2466, 'ली।': 2467, 'लूँ': 2468, 'लूँगा।': 2469, 'लूँ।': 2470, 'ले': 2471, 'लें': 2472, 'लेंगे': 2473, 'लेआऊँ': 2474, 'लेकर': 2475, 'लेकिन': 2476, 'लेखक': 2477, 'लेगा।': 2478, 'लेजाओगे': 2479, 'लेजाती': 2480, 'लेजाने': 2481, 'लेट': 2482, 'लेटा': 2483, 'लेटी': 2484, 'लेता': 2485, 'लेते': 2486, 'लेना': 2487, 'लेनादेना': 2488, 'लेना।': 2489, 'लेनी': 2490, 'लेनीं': 2491, 'लेने': 2492, 'लेलिया।': 2493, 'लेलूँगा।': 2494, 'लेलो।': 2495, 'ले।': 2496, 'लो': 2497, 'लोकतंत्र': 2498, 'लोकप्रिय': 2499, 'लोग': 2500, 'लोगे': 2501, 'लोगों': 2502, 'लोमड़ी': 2503, 'लोहा': 2504, 'लो।': 2505, 'लौट': 2506, 'लौटकर': 2507, 'लौटाए।': 2508, 'लौटाना': 2509, 'लौटाने': 2510, 'वक्त': 2511, 'वजह': 2512, 'वफ़ादारी': 2513, 'वर्तमान': 2514, 'वर्ष': 2515, 'वर्षा': 2516, 'वसीयत': 2517, 'वस्तु': 2518, 'वह': 2519, 'वहाँ': 2520, 'वहां': 2521, 'वही': 2522, 'वाईन': 2523, 'वाक्य': 2524, 'वाक्यांश': 2525, 'वाक्यांशों': 2526, 'वाक्यों': 2527, 'वातावरण': 2528, 'वादा': 2529, 'वापस': 2530, 'वार': 2531, 'वाला': 2532, 'वाली': 2533, 'वाले': 2534, 'वालो': 2535, 'वाशिंगटन': 2536, 'वाह': 2537, 'वाहवाह': 2538, 'विख्यात': 2539, 'विचार': 2540, 'विज्ञान': 2541, 'विदेश': 2542, 'विदेशी': 2543, 'विदेषी': 2544, 'विदोश': 2545, 'विद्यार्थी': 2546, 'विद्रोह': 2547, 'विमान': 2548, 'विरोध': 2549, 'विरोधी': 2550, 'विलोम': 2551, 'विशाल': 2552, 'विश्व': 2553, 'विश्वविद्यालय': 2554, 'विश्वास': 2555, 'विषय': 2556, 'विष्व': 2557, 'विस्तृत': 2558, 'वे': 2559, 'वेतन': 2560, 'वैश्विक': 2561, 'वैसा': 2562, 'वैसी': 2563, 'वैसे': 2564, 'वॉशिंग': 2565, 'वो': 2566, 'व्यक्ति': 2567, 'व्यस्थ': 2568, 'व्यापार': 2569, 'शक': 2570, 'शक़': 2571, 'शतक': 2572, 'शनिवार': 2573, 'शब्द': 2574, 'शब्दकोश': 2575, 'शब्दकोष': 2576, 'शब्दों': 2577, 'शराब': 2578, 'शराबख़ाना': 2579, 'शर्म': 2580, 'शहर': 2581, 'शहरों': 2582, 'शांति': 2583, 'शादी': 2584, 'शादीशुदा': 2585, 'शानदार': 2586, 'शाबाश': 2587, 'शाम': 2588, 'शामिल': 2589, 'शायद': 2590, 'शास्त्रीय': 2591, 'शिक़ायत': 2592, 'शिकायत': 2593, 'शिकार': 2594, 'शिक्षा': 2595, 'शिखर': 2596, 'शुक्रवार': 2597, 'शुभकामनाएं': 2598, 'शुरु': 2599, 'शुरू': 2600, 'शेर': 2601, 'शेरों': 2602, 'शोक': 2603, 'शोर': 2604, 'शौक': 2605, 'शौकिया': 2606, 'शौकीन': 2607, 'संगीत': 2608, 'संतुलन': 2609, 'संतुष्ट': 2610, 'संत्रे': 2611, 'संदेश': 2612, 'संपत्ति': 2613, 'संबंध': 2614, 'संभवतः': 2615, 'संभालने': 2616, 'संयुक्त': 2617, 'संविधान': 2618, 'सकता': 2619, 'सकता।': 2620, 'सकती': 2621, 'सकतीं': 2622, 'सकती।': 2623, 'सकते': 2624, 'सकतें': 2625, 'सकते।': 2626, 'सका': 2627, 'सका।': 2628, 'सके': 2629, 'सके।': 2630, 'सको': 2631, 'सख्त': 2632, 'सगाई': 2633, 'सच': 2634, 'सच्चा': 2635, 'सच्चाई': 2636, 'सच्ची': 2637, 'सज़ा': 2638, 'सड़': 2639, 'सड़क': 2640, 'सड़ने': 2641, 'सड़े': 2642, 'सत्रह': 2643, 'सदस्य': 2644, 'सदस्यों': 2645, 'सन': 2646, 'सपना': 2647, 'सपने': 2648, 'सप्ताह': 2649, 'सफ़र': 2650, 'सफ़ल': 2651, 'सफ़ाई': 2652, 'सफ़ेद': 2653, 'सब': 2654, 'सबका': 2655, 'सबकुछ': 2656, 'सबसे': 2657, 'सब्ज़ियाँ': 2658, 'सब्र': 2659, 'सभी': 2660, 'समझ': 2661, 'समझकर': 2662, 'समझता': 2663, 'समझते': 2664, 'समझदार': 2665, 'समझना': 2666, 'समझने': 2667, 'समझा': 2668, 'समझाई।': 2669, 'समझाउँगा।': 2670, 'समझाना': 2671, 'समझाया।': 2672, 'समझिए।': 2673, 'समझे': 2674, 'समझो।': 2675, 'समय': 2676, 'समर्पित': 2677, 'समस्या': 2678, 'समाचार': 2679, 'समान': 2680, 'समानताएं': 2681, 'समाप्त': 2682, 'समिति': 2683, 'समुंदर': 2684, 'समुद्र': 2685, 'सम्पर्क': 2686, 'सम्भाल': 2687, 'सम्मान': 2688, 'सम्मेलन': 2689, 'सर': 2690, 'सरकार': 2691, 'सरकारों': 2692, 'सरल': 2693, 'सरासर': 2694, 'सर्दी': 2695, 'सर्वोत्तम': 2696, 'सलह': 2697, 'सलाह': 2698, 'सवाल': 2699, 'सवालों': 2700, 'सस्ता': 2701, 'सस्ते': 2702, 'सहन': 2703, 'सहने': 2704, 'सहपाठियों': 2705, 'सहमत': 2706, 'सहायता': 2707, 'सही': 2708, 'सहीग़लत': 2709, 'सा': 2710, 'साँस': 2711, 'सांप': 2712, 'सांस': 2713, 'साईकल': 2714, 'साठ': 2715, 'साढ़े': 2716, 'साढ़ेआठ': 2717, 'सात': 2718, 'सातवाँ': 2719, 'साथ': 2720, 'साथतानी': 2721, 'साधा': 2722, 'साफ़': 2723, 'साफ़सुथरा': 2724, 'साबित': 2725, 'सामने': 2726, 'सामान': 2727, 'सायकल': 2728, 'सारा': 2729, 'सारी': 2730, 'सारीकीसारी': 2731, 'सारे': 2732, 'साल': 2733, 'सालों': 2734, 'सावधान': 2735, 'साहब': 2736, 'साहित्य': 2737, 'सिकुड़': 2738, 'सिक्कों': 2739, 'सिखा': 2740, 'सिखाए': 2741, 'सिखाया।': 2742, 'सिगरेट': 2743, 'सितारे': 2744, 'सिद्धांत': 2745, 'सिनेमा': 2746, 'सिपाहियों': 2747, 'सिर': 2748, 'सिरदर्द': 2749, 'सिवाय': 2750, 'सिस्टर': 2751, 'सी': 2752, 'सीख': 2753, 'सीखना': 2754, 'सीखने': 2755, 'सीखा।': 2756, 'सीखी': 2757, 'सीटें': 2758, 'सीड़ियों': 2759, 'सीढ़ियों': 2760, 'सीधी': 2761, 'सीधे': 2762, 'सीधेसीधे': 2763, 'सीमा': 2764, 'सुँघाई': 2765, 'सुंदर': 2766, 'सुंदरता': 2767, 'सुख': 2768, 'सुझाव': 2769, 'सुधार': 2770, 'सुधारना': 2771, 'सुधारने': 2772, 'सुधारिए।': 2773, 'सुन': 2774, 'सुनकर': 2775, 'सुनता': 2776, 'सुनते': 2777, 'सुनने': 2778, 'सुना': 2779, 'सुनाई': 2780, 'सुनाओ।': 2781, 'सुनाया': 2782, 'सुना।': 2783, 'सुनिए': 2784, 'सुनी': 2785, 'सुनी।': 2786, 'सुनेगा।': 2787, 'सुन्दर': 2788, 'सुबह': 2789, 'सुरक्षा': 2790, 'सुलझाना': 2791, 'सुलाना': 2792, 'सुशील': 2793, 'सूँघ': 2794, 'सूखा': 2795, 'सूज': 2796, 'सूझी': 2797, 'सूट': 2798, 'सूप': 2799, 'सूपरमार्केट': 2800, 'सूरज': 2801, 'सूर्य': 2802, 'से': 2803, 'सेइचो': 2804, 'सेब': 2805, 'सेव': 2806, 'सेवाओं': 2807, 'सेहत': 2808, 'सैंडविच': 2809, 'सैर': 2810, 'सो': 2811, 'सोकर': 2812, 'सोच': 2813, 'सोचता': 2814, 'सोचना': 2815, 'सोचने': 2816, 'सोचा': 2817, 'सोचा।': 2818, 'सोचो': 2819, 'सोते': 2820, 'सोत्रों': 2821, 'सोना': 2822, 'सोनी': 2823, 'सोने': 2824, 'सोफ़ा': 2825, 'सोमवार': 2826, 'सोये': 2827, 'सोलहवा': 2828, 'सौ': 2829, 'सौंप': 2830, 'स्कूल': 2831, 'स्केटिंग': 2832, 'स्टेशन': 2833, 'स्टॉप': 2834, 'स्तिथि': 2835, 'स्थापित': 2836, 'स्थित': 2837, 'स्पा': 2838, 'स्पैनिश': 2839, 'स्मॉग': 2840, 'स्याही': 2841, 'स्रोत': 2842, 'स्वतंत्रता': 2843, 'स्वयं': 2844, 'स्वागत': 2845, 'स्वागतम्।': 2846, 'स्वाद': 2847, 'स्वास्थ्य': 2848, 'स्विजरलैंड': 2849, 'स्वीकार': 2850, 'स्वेटर': 2851, 'हँस': 2852, 'हँसा।': 2853, 'हँसे।': 2854, 'हकला': 2855, 'हज़ार': 2856, 'हज़ारों': 2857, 'हटा': 2858, 'हड़ताल': 2859, 'हड़बड़ी': 2860, 'हड्डी': 2861, 'हत्याएँ': 2862, 'हद': 2863, 'हफ़्ते': 2864, 'हफ़्तों': 2865, 'हम': 2866, 'हमको': 2867, 'हमने': 2868, 'हमला': 2869, 'हमसे': 2870, 'हमारा': 2871, 'हमारी': 2872, 'हमारे': 2873, 'हमे': 2874, 'हमें': 2875, 'हमेशा': 2876, 'हर': 2877, 'हरा': 2878, 'हरासत': 2879, 'हरे': 2880, 'हल': 2881, 'हल्का': 2882, 'हल्की': 2883, 'हल्के': 2884, 'हवा': 2885, 'हवाई': 2886, 'हवाईअड्डे': 2887, 'हवाईजहाज़': 2888, 'हसीना': 2889, 'हस्ताक्षर': 2890, 'हस्पताल': 2891, 'हाँ': 2892, 'हाँ।': 2893, 'हाथ': 2894, 'हाथपैर': 2895, 'हाथी': 2896, 'हादसा': 2897, 'हादसे': 2898, 'हानि': 2899, 'हाफ़िज़।': 2900, 'हार': 2901, 'हाल': 2902, 'हालत': 2903, 'हालांकि': 2904, 'हिचकिचाएँ।': 2905, 'हिचकिचाया।': 2906, 'हिचकिचाहट': 2907, 'हित': 2908, 'हिफ़ाज़त': 2909, 'हिरण': 2910, 'हिरासत': 2911, 'हिल': 2912, 'हिला': 2913, 'हिलो': 2914, 'हिसाब': 2915, 'हिस्से': 2916, 'हिस्सो': 2917, 'ही': 2918, 'हीरा': 2919, 'हीरों': 2920, 'ही।': 2921, 'हुआ': 2922, 'हुआ।': 2923, 'हुई': 2924, 'हुईं': 2925, 'हुई।': 2926, 'हुए': 2927, 'हुएँ': 2928, 'हुए।': 2929, 'हुक़्म': 2930, 'हुकूमत': 2931, 'हूँ': 2932, 'हूँगा': 2933, 'हूँ।': 2934, 'हे': 2935, 'हेलमेट': 2936, 'है': 2937, 'हैं': 2938, 'हैं।': 2939, 'हैरान': 2940, 'है।': 2941, 'हॉल': 2942, 'हो': 2943, 'हों': 2944, 'होंगीं': 2945, 'होंगे': 2946, 'होंगे।': 2947, 'होएगी।': 2948, 'होकर': 2949, 'होक्काईदो': 2950, 'होगा': 2951, 'होगा।': 2952, 'होगी': 2953, 'होगी।': 2954, 'होगे': 2955, 'होटल': 2956, 'होता': 2957, 'होता।': 2958, 'होती': 2959, 'होतीं': 2960, 'होती।': 2961, 'होते': 2962, 'होना': 2963, 'होनी': 2964, 'होने': 2965, 'होमवर्क': 2966, 'हो।': 2967, '।': 2968}\n"
     ]
    }
   ],
   "source": [
    "print(input_token_index)\n",
    "print(target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7you-9lLjv_"
   },
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "jY1naWuiL_0P",
    "outputId": "52950473-1972-4d5e-9ec6-fa683fc91d6a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hin</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>START_ तुम नसीब वालो हो जिसके इतना प्यार करने ...</td>\n",
       "      <td>you are fortunate to have such loving parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>START_ मोटे मत हो जाना। _END</td>\n",
       "      <td>dont get fat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>START_ जो अखबार में लिखा है वह सच है। _END</td>\n",
       "      <td>what the newspapers say is true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>START_ यह सही नहीं है। _END</td>\n",
       "      <td>this isnt right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>START_ उसकी राष्ट्रीयता क्या है _END</td>\n",
       "      <td>what is his nationality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>START_ वह हर सुबह अखबार पढ़ता है। _END</td>\n",
       "      <td>he reads the paper every morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>START_ नई सरकार ने देश को भ्रष्टाचार से मुक्त ...</td>\n",
       "      <td>the new government promised to rid the country...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>START_ पता नहीं कैसे पर तुमने कर दिया। _END</td>\n",
       "      <td>i dont know how but you did it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>START_ उसने कपड़े उतारे बिना ही पानी में छलाँग...</td>\n",
       "      <td>he jumped into the water clothes and all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>START_ कृपया ध्यान दीजिए। _END</td>\n",
       "      <td>give me your attention please</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    hin  \\\n",
       "2496  START_ तुम नसीब वालो हो जिसके इतना प्यार करने ...   \n",
       "113                        START_ मोटे मत हो जाना। _END   \n",
       "1652         START_ जो अखबार में लिखा है वह सच है। _END   \n",
       "277                         START_ यह सही नहीं है। _END   \n",
       "862                START_ उसकी राष्ट्रीयता क्या है _END   \n",
       "1673             START_ वह हर सुबह अखबार पढ़ता है। _END   \n",
       "2729  START_ नई सरकार ने देश को भ्रष्टाचार से मुक्त ...   \n",
       "1682        START_ पता नहीं कैसे पर तुमने कर दिया। _END   \n",
       "2264  START_ उसने कपड़े उतारे बिना ही पानी में छलाँग...   \n",
       "1501                     START_ कृपया ध्यान दीजिए। _END   \n",
       "\n",
       "                                                    eng  \n",
       "2496      you are fortunate to have such loving parents  \n",
       "113                                        dont get fat  \n",
       "1652                    what the newspapers say is true  \n",
       "277                                     this isnt right  \n",
       "862                             what is his nationality  \n",
       "1673                   he reads the paper every morning  \n",
       "2729  the new government promised to rid the country...  \n",
       "1682                     i dont know how but you did it  \n",
       "2264           he jumped into the water clothes and all  \n",
       "1501                      give me your attention please  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IGONBTluMKCH",
    "outputId": "933c54b3-2876-4bc1-ede5-8e0e6df74de1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2495,), (278,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = lines.eng, lines.hin\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "53At5wtGVwPa",
    "outputId": "009b8078-a5c8-40cb-9f44-a8bf8a44ce98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2773"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "lOyDhgj7yfFo",
    "outputId": "219dd807-27ed-4d93-d447-7f65dee5d02d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585                        START_ तुम कब लौट कर आओगे _END\n",
       "198                           START_ सब लोग कैसे हैं _END\n",
       "77                                START_ तुम कैसे हो _END\n",
       "350                           START_ तू कहाँ रहता है _END\n",
       "744             START_ स्कूल साढ़े तीन बजे छूटता है। _END\n",
       "269                      START_ आपसे मिलकर खुशी हुई। _END\n",
       "1885       START_ मैं पियानो बहुत अच्छा बजा लेता था। _END\n",
       "2151    START_ उसने अपनी गाड़ी बिना हिचकिचाहट हे बेच द...\n",
       "561                          START_ यह मुफ़्त का है। _END\n",
       "1478                   START_ पत्र किसको लिखा गया था _END\n",
       "Name: hin, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "3RntoaSUympO",
    "outputId": "c3d72556-dd1a-43ea-cc16-f1bcb86ef99c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585                       when will you return\n",
       "198                            how is everyone\n",
       "77                                 how are you\n",
       "350                          where do you live\n",
       "744                          school is over at\n",
       "269                           nice to meet you\n",
       "1885        i was able to play piano very well\n",
       "2151    he sold his own car without hesitation\n",
       "561                       it is free of charge\n",
       "1478             who was the letter written to\n",
       "Name: eng, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3E9fCaFPRYz"
   },
   "outputs": [],
   "source": [
    "X_train.to_pickle('/content/drive/My Drive/Colab Notebooks/LanguageTranslation/Weights_Mar/X_train.pkl')\n",
    "X_test.to_pickle('/content/drive/My Drive/Colab Notebooks/LanguageTranslation/Weights_Mar/X_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uh5m7tqYNNc9"
   },
   "source": [
    "# Encoder - Decoder data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMNxEerS1np4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input shape (2773, 22) decoder input shape (2773, 27) decoder out shape (2773, 27, 2969)\n",
      "target START_\n",
      "target तुम\n",
      "target नसीब\n",
      "target वालो\n",
      "target हो\n",
      "target जिसके\n",
      "target इतना\n" 
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(X), max_length_src), #using all data becaus the dataset is small but if we have bigger dataset we can we use split data as well\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(X), max_length_tar),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(X), max_length_tar, num_decoder_tokens), #we can  also pass the pass y_train bcoz we just need len nothing else\n",
    "    dtype='float32')\n",
    "\n",
    "   \n",
    "print(\"encoder input shape\",encoder_input_data.shape,'decoder input shape',decoder_input_data.shape,'decoder out shape',decoder_target_data.shape)\n",
    "for i, (input_text, target_text) in enumerate(zip(X, y)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "            encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "            print('target',word)\n",
    "            if t<len(target_text.split())-1:\n",
    "                decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "            if t>0:\n",
    "              # decoder target sequence (one hot encoded)\n",
    "              # does not include the START_ token\n",
    "              # Offset by one timestep\n",
    "              decoder_target_data[i, t - 1, target_token_index[word]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 1.219e+03 1.456e+03 2.535e+03 2.943e+03 1.052e+03 2.080e+02\n",
      "  1.744e+03 4.210e+02 2.534e+03 2.169e+03 2.939e+03 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [1.000e+00 2.268e+03 2.126e+03 2.943e+03 1.032e+03 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]]\n",
      "2496    START_ तुम नसीब वालो हो जिसके इतना प्यार करने ...\n",
      "113                          START_ मोटे मत हो जाना। _END\n",
      "Name: hin, dtype: object\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input_data[:2])\n",
    "print(y[:2])\n",
    "target_token_index[\"हो\"]\n",
    "print(decoder_target_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hVI9yq5FjiK"
   },
   "outputs": [],
   "source": [
    "#HERE THE DATASET IS SO SMALL WE CAN USE THIS CODE FOR GENERATING VALIDATING MATRICES FOR INPUT\n",
    "\n",
    "'''encoder_input_val_data = np.zeros(\n",
    "    (len(X_test), max_length_src),\n",
    "    dtype='float32')\n",
    "decoder_input_val_data = np.zeros(\n",
    "    (len(X_test), max_length_tar),\n",
    "    dtype='float32')\n",
    "decoder_target_val_data = np.zeros(\n",
    "    (len(X_test), max_length_tar, num_decoder_tokens), #we can  also pass the pass y_train bcoz we just need len nothing else\n",
    "    dtype='float32')\n",
    "\n",
    "\n",
    "   \n",
    "for i, (input_text, target_text) in enumerate(zip(X_test, y_test)):\n",
    "      for t, word in enumerate(input_text.split()):\n",
    "          encoder_input_val_data[i, t] = input_token_index[word] # encoder input seq\n",
    "      for t, word in enumerate(target_text.split()):\n",
    "          if t<len(target_text.split())-1:\n",
    "              decoder_input_val_data[i, t] = target_token_index[word] # decoder input seq\n",
    "          if t>0:\n",
    "              # decoder target sequence (one hot encoded)\n",
    "              # does not include the START_ token\n",
    "              # Offset by one timestep\n",
    "              decoder_target_val_data[i, t - 1, target_token_index[word]] = 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "77xj-et1F5EB",
    "outputId": "6c2da0f0-4b64-42ef-e1aa-00b59b40d338"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 22)"
      ]
     },
     "execution_count": 197,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder_input_val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "abKzUBW0GAUR",
    "outputId": "ccb692a1-7fe7-4493-d7a1-902520d7098a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 27, 2969)"
      ]
     },
     "execution_count": 201,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decoder_target_val_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ADtxy8cnAcb_",
    "outputId": "5fdb98d5-6104-4461-b66e-739f2281bed7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2773, 22)"
      ]
     },
     "execution_count": 320,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decoder_input_data[0].shape\n",
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER DECODER MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWQTGqJtcCFv"
   },
   "outputs": [],
   "source": [
    "#ENCODER DECODER MODEL ARCHITECTURE\n",
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkuSjt_TSquR"
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ld_AYIZwHKcC"
   },
   "outputs": [],
   "source": [
    "#Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], \n",
    "              s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLEHLs_8IW2b"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    599552      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    760064      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 525312      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2969)   763033      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,173,273\n",
      "Trainable params: 3,173,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vI34QFAdpLen"
   },
   "outputs": [],
   "source": [
    "#displaying model architecture\n",
    "#from IPython.display import Image\n",
    "#Image(retina=True, filename='train_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjKSaaqYpVHG"
   },
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 64\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lIhYTIeH7Cti",
    "outputId": "f9c8076c-1d34-4e7d-9b74-e074ad7ee8c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278,)"
      ]
     },
     "execution_count": 255,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CX93OI38w2S-",
    "outputId": "a2dc9cc5-9c5a-48ae-cea7-6bbcca6c1d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 1.4512 - acc: 0.1812 - val_loss: 1.4017 - val_acc: 0.1841\n",
      "Epoch 2/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 1.4022 - acc: 0.1911 - val_loss: 1.3647 - val_acc: 0.1901\n",
      "Epoch 3/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 1.3569 - acc: 0.2034 - val_loss: 1.3312 - val_acc: 0.2074\n",
      "Epoch 4/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 1.3149 - acc: 0.2162 - val_loss: 1.3029 - val_acc: 0.2153\n",
      "Epoch 5/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 1.2738 - acc: 0.2286 - val_loss: 1.2692 - val_acc: 0.2302\n",
      "Epoch 6/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 1.2339 - acc: 0.2387 - val_loss: 1.2469 - val_acc: 0.2432\n",
      "Epoch 7/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 1.1963 - acc: 0.2530 - val_loss: 1.2071 - val_acc: 0.2549\n",
      "Epoch 8/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 1.1589 - acc: 0.2651 - val_loss: 1.1814 - val_acc: 0.2549\n",
      "Epoch 9/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 1.1231 - acc: 0.2794 - val_loss: 1.1543 - val_acc: 0.2647\n",
      "Epoch 10/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 1.0880 - acc: 0.2898 - val_loss: 1.1160 - val_acc: 0.2861\n",
      "Epoch 11/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 1.0540 - acc: 0.3029 - val_loss: 1.0916 - val_acc: 0.3015\n",
      "Epoch 12/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 1.0207 - acc: 0.3153 - val_loss: 1.0647 - val_acc: 0.3136\n",
      "Epoch 13/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.9882 - acc: 0.3282 - val_loss: 1.0403 - val_acc: 0.3234\n",
      "Epoch 14/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.9566 - acc: 0.3398 - val_loss: 1.0103 - val_acc: 0.3430\n",
      "Epoch 15/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.9231 - acc: 0.3585 - val_loss: 0.9864 - val_acc: 0.3486\n",
      "Epoch 16/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.8925 - acc: 0.3724 - val_loss: 0.9591 - val_acc: 0.3565\n",
      "Epoch 17/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.8609 - acc: 0.3872 - val_loss: 0.9341 - val_acc: 0.3816\n",
      "Epoch 18/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.8306 - acc: 0.4031 - val_loss: 0.9141 - val_acc: 0.3928\n",
      "Epoch 19/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.8013 - acc: 0.4207 - val_loss: 0.8913 - val_acc: 0.4017\n",
      "Epoch 20/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.7700 - acc: 0.4398 - val_loss: 0.8679 - val_acc: 0.4226\n",
      "Epoch 21/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.7413 - acc: 0.4590 - val_loss: 0.8449 - val_acc: 0.4506\n",
      "Epoch 22/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.7122 - acc: 0.4773 - val_loss: 0.8191 - val_acc: 0.4660\n",
      "Epoch 23/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.6844 - acc: 0.4994 - val_loss: 0.7996 - val_acc: 0.4730\n",
      "Epoch 24/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.6568 - acc: 0.5206 - val_loss: 0.7800 - val_acc: 0.5130\n",
      "Epoch 25/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.6277 - acc: 0.5424 - val_loss: 0.7608 - val_acc: 0.5098\n",
      "Epoch 26/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.6010 - acc: 0.5624 - val_loss: 0.7415 - val_acc: 0.5238\n",
      "Epoch 27/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.5748 - acc: 0.5829 - val_loss: 0.7146 - val_acc: 0.5606\n",
      "Epoch 28/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.5472 - acc: 0.6098 - val_loss: 0.6972 - val_acc: 0.5746\n",
      "Epoch 29/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.5212 - acc: 0.6339 - val_loss: 0.6804 - val_acc: 0.5857\n",
      "Epoch 30/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.4965 - acc: 0.6541 - val_loss: 0.6652 - val_acc: 0.6021\n",
      "Epoch 31/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.4699 - acc: 0.6754 - val_loss: 0.6497 - val_acc: 0.6053\n",
      "Epoch 32/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.4463 - acc: 0.6994 - val_loss: 0.6217 - val_acc: 0.6412\n",
      "Epoch 33/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.4216 - acc: 0.7190 - val_loss: 0.6120 - val_acc: 0.6468\n",
      "Epoch 34/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.3988 - acc: 0.7358 - val_loss: 0.6036 - val_acc: 0.6636\n",
      "Epoch 35/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.3767 - acc: 0.7592 - val_loss: 0.5770 - val_acc: 0.6803\n",
      "Epoch 36/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.3550 - acc: 0.7790 - val_loss: 0.5581 - val_acc: 0.7078\n",
      "Epoch 37/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.3318 - acc: 0.7985 - val_loss: 0.5450 - val_acc: 0.7144\n",
      "Epoch 38/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.3133 - acc: 0.8129 - val_loss: 0.5305 - val_acc: 0.7255\n",
      "Epoch 39/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.2910 - acc: 0.8316 - val_loss: 0.5244 - val_acc: 0.7390\n",
      "Epoch 40/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.2747 - acc: 0.8440 - val_loss: 0.5055 - val_acc: 0.7563\n",
      "Epoch 41/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.2546 - acc: 0.8606 - val_loss: 0.4911 - val_acc: 0.7623\n",
      "Epoch 42/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.2364 - acc: 0.8762 - val_loss: 0.4775 - val_acc: 0.7768\n",
      "Epoch 43/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.2200 - acc: 0.8885 - val_loss: 0.4688 - val_acc: 0.7838\n",
      "Epoch 44/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.2046 - acc: 0.8967 - val_loss: 0.4587 - val_acc: 0.7959\n",
      "Epoch 45/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.1882 - acc: 0.9118 - val_loss: 0.4449 - val_acc: 0.7996\n",
      "Epoch 46/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.1742 - acc: 0.9200 - val_loss: 0.4405 - val_acc: 0.8127\n",
      "Epoch 47/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.1615 - acc: 0.9298 - val_loss: 0.4295 - val_acc: 0.8127\n",
      "Epoch 48/200\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.1467 - acc: 0.9369 - val_loss: 0.4196 - val_acc: 0.8187\n",
      "Epoch 49/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.1352 - acc: 0.9440 - val_loss: 0.4135 - val_acc: 0.8248\n",
      "Epoch 50/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.1236 - acc: 0.9507 - val_loss: 0.4065 - val_acc: 0.8295\n",
      "Epoch 51/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.1123 - acc: 0.9583 - val_loss: 0.4022 - val_acc: 0.8285\n",
      "Epoch 52/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.1029 - acc: 0.9631 - val_loss: 0.3964 - val_acc: 0.8336\n",
      "Epoch 53/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0931 - acc: 0.9687 - val_loss: 0.3918 - val_acc: 0.8392\n",
      "Epoch 54/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0842 - acc: 0.9716 - val_loss: 0.3920 - val_acc: 0.8378\n",
      "Epoch 55/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0766 - acc: 0.9748 - val_loss: 0.3813 - val_acc: 0.8416\n",
      "Epoch 56/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0696 - acc: 0.9777 - val_loss: 0.3780 - val_acc: 0.8416\n",
      "Epoch 57/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0623 - acc: 0.9804 - val_loss: 0.3741 - val_acc: 0.8430\n",
      "Epoch 58/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0565 - acc: 0.9811 - val_loss: 0.3726 - val_acc: 0.8458\n",
      "Epoch 59/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0513 - acc: 0.9828 - val_loss: 0.3726 - val_acc: 0.8430\n",
      "Epoch 60/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0466 - acc: 0.9843 - val_loss: 0.3667 - val_acc: 0.8472\n",
      "Epoch 61/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0415 - acc: 0.9845 - val_loss: 0.3635 - val_acc: 0.8448\n",
      "Epoch 62/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0376 - acc: 0.9862 - val_loss: 0.3627 - val_acc: 0.8472\n",
      "Epoch 63/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0345 - acc: 0.9864 - val_loss: 0.3619 - val_acc: 0.8481\n",
      "Epoch 64/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0302 - acc: 0.9869 - val_loss: 0.3619 - val_acc: 0.8448\n",
      "Epoch 65/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0281 - acc: 0.9871 - val_loss: 0.3605 - val_acc: 0.8500\n",
      "Epoch 66/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0254 - acc: 0.9873 - val_loss: 0.3598 - val_acc: 0.8495\n",
      "Epoch 67/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0230 - acc: 0.9878 - val_loss: 0.3592 - val_acc: 0.8509\n",
      "Epoch 68/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0213 - acc: 0.9877 - val_loss: 0.3632 - val_acc: 0.8444\n",
      "Epoch 69/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0193 - acc: 0.9883 - val_loss: 0.3594 - val_acc: 0.8486\n",
      "Epoch 70/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0178 - acc: 0.9879 - val_loss: 0.3624 - val_acc: 0.8481\n",
      "Epoch 71/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0164 - acc: 0.9879 - val_loss: 0.3631 - val_acc: 0.8476\n",
      "Epoch 72/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0156 - acc: 0.9877 - val_loss: 0.3636 - val_acc: 0.8444\n",
      "Epoch 73/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0143 - acc: 0.9883 - val_loss: 0.3655 - val_acc: 0.8495\n",
      "Epoch 74/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0133 - acc: 0.9878 - val_loss: 0.3658 - val_acc: 0.8476\n",
      "Epoch 75/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0127 - acc: 0.9882 - val_loss: 0.3634 - val_acc: 0.8467\n",
      "Epoch 76/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0118 - acc: 0.9886 - val_loss: 0.3663 - val_acc: 0.8458\n",
      "Epoch 77/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0113 - acc: 0.9884 - val_loss: 0.3674 - val_acc: 0.8490\n",
      "Epoch 78/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0106 - acc: 0.9884 - val_loss: 0.3707 - val_acc: 0.8462\n",
      "Epoch 79/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0103 - acc: 0.9884 - val_loss: 0.3696 - val_acc: 0.8448\n",
      "Epoch 80/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0099 - acc: 0.9886 - val_loss: 0.3691 - val_acc: 0.8500\n",
      "Epoch 81/200\n",
      "35/35 [==============================] - 2s 49ms/step - loss: 0.0095 - acc: 0.9882 - val_loss: 0.3713 - val_acc: 0.8467\n",
      "Epoch 82/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0090 - acc: 0.9887 - val_loss: 0.3721 - val_acc: 0.8476\n",
      "Epoch 83/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0088 - acc: 0.9883 - val_loss: 0.3746 - val_acc: 0.8444\n",
      "Epoch 84/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0085 - acc: 0.9885 - val_loss: 0.3739 - val_acc: 0.8453\n",
      "Epoch 85/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0082 - acc: 0.9884 - val_loss: 0.3772 - val_acc: 0.8486\n",
      "Epoch 86/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0080 - acc: 0.9889 - val_loss: 0.3781 - val_acc: 0.8444\n",
      "Epoch 87/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0077 - acc: 0.9886 - val_loss: 0.3787 - val_acc: 0.8448\n",
      "Epoch 88/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0076 - acc: 0.9881 - val_loss: 0.3778 - val_acc: 0.8472\n",
      "Epoch 89/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0074 - acc: 0.9886 - val_loss: 0.3814 - val_acc: 0.8500\n",
      "Epoch 90/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0073 - acc: 0.9880 - val_loss: 0.3820 - val_acc: 0.8462\n",
      "Epoch 91/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0073 - acc: 0.9882 - val_loss: 0.3777 - val_acc: 0.8486\n",
      "Epoch 92/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0071 - acc: 0.9886 - val_loss: 0.3815 - val_acc: 0.8481\n",
      "Epoch 93/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0070 - acc: 0.9882 - val_loss: 0.3827 - val_acc: 0.8462\n",
      "Epoch 94/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0069 - acc: 0.9886 - val_loss: 0.3862 - val_acc: 0.8439\n",
      "Epoch 95/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0067 - acc: 0.9885 - val_loss: 0.3896 - val_acc: 0.8458\n",
      "Epoch 96/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0066 - acc: 0.9887 - val_loss: 0.3871 - val_acc: 0.8444\n",
      "Epoch 97/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0065 - acc: 0.9889 - val_loss: 0.3931 - val_acc: 0.8481\n",
      "Epoch 98/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0065 - acc: 0.9891 - val_loss: 0.3896 - val_acc: 0.8481\n",
      "Epoch 99/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0066 - acc: 0.9880 - val_loss: 0.3897 - val_acc: 0.8481\n",
      "Epoch 100/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0064 - acc: 0.9885 - val_loss: 0.3903 - val_acc: 0.8453\n",
      "Epoch 101/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0066 - acc: 0.9882 - val_loss: 0.3918 - val_acc: 0.8448\n",
      "Epoch 102/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0062 - acc: 0.9885 - val_loss: 0.3921 - val_acc: 0.8472\n",
      "Epoch 103/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0063 - acc: 0.9885 - val_loss: 0.3957 - val_acc: 0.8453\n",
      "Epoch 104/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0063 - acc: 0.9886 - val_loss: 0.3913 - val_acc: 0.8476\n",
      "Epoch 105/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0062 - acc: 0.9882 - val_loss: 0.3946 - val_acc: 0.8476\n",
      "Epoch 106/200\n",
      "35/35 [==============================] - 2s 48ms/step - loss: 0.0061 - acc: 0.9888 - val_loss: 0.3947 - val_acc: 0.8481\n",
      "Epoch 107/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0059 - acc: 0.9893 - val_loss: 0.3981 - val_acc: 0.8458\n",
      "Epoch 108/200\n",
      "35/35 [==============================] - 2s 49ms/step - loss: 0.0060 - acc: 0.9884 - val_loss: 0.3975 - val_acc: 0.8458\n",
      "Epoch 109/200\n",
      "35/35 [==============================] - 2s 48ms/step - loss: 0.0058 - acc: 0.9884 - val_loss: 0.3982 - val_acc: 0.8462\n",
      "Epoch 110/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0060 - acc: 0.9888 - val_loss: 0.3987 - val_acc: 0.8458\n",
      "Epoch 111/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0059 - acc: 0.9884 - val_loss: 0.4018 - val_acc: 0.8434\n",
      "Epoch 112/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0058 - acc: 0.9886 - val_loss: 0.4002 - val_acc: 0.8458\n",
      "Epoch 113/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0058 - acc: 0.9884 - val_loss: 0.4026 - val_acc: 0.8439\n",
      "Epoch 114/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0058 - acc: 0.9884 - val_loss: 0.4031 - val_acc: 0.8453\n",
      "Epoch 115/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0058 - acc: 0.9880 - val_loss: 0.4043 - val_acc: 0.8458\n",
      "Epoch 116/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0057 - acc: 0.9883 - val_loss: 0.4063 - val_acc: 0.8444\n",
      "Epoch 117/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0056 - acc: 0.9886 - val_loss: 0.4089 - val_acc: 0.8476\n",
      "Epoch 118/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0057 - acc: 0.9887 - val_loss: 0.4066 - val_acc: 0.8481\n",
      "Epoch 119/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0057 - acc: 0.9888 - val_loss: 0.4100 - val_acc: 0.8462\n",
      "Epoch 120/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0056 - acc: 0.9889 - val_loss: 0.4078 - val_acc: 0.8420\n",
      "Epoch 121/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0056 - acc: 0.9882 - val_loss: 0.4087 - val_acc: 0.8481\n",
      "Epoch 122/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0057 - acc: 0.9885 - val_loss: 0.4100 - val_acc: 0.8453\n",
      "Epoch 123/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0056 - acc: 0.9884 - val_loss: 0.4090 - val_acc: 0.8486\n",
      "Epoch 124/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0055 - acc: 0.9881 - val_loss: 0.4143 - val_acc: 0.8495\n",
      "Epoch 125/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0058 - acc: 0.9885 - val_loss: 0.4164 - val_acc: 0.8476\n",
      "Epoch 126/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0055 - acc: 0.9885 - val_loss: 0.4158 - val_acc: 0.8453\n",
      "Epoch 127/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0055 - acc: 0.9879 - val_loss: 0.4177 - val_acc: 0.8448\n",
      "Epoch 128/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0055 - acc: 0.9890 - val_loss: 0.4145 - val_acc: 0.8472\n",
      "Epoch 129/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0055 - acc: 0.9882 - val_loss: 0.4153 - val_acc: 0.8490\n",
      "Epoch 130/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0054 - acc: 0.9884 - val_loss: 0.4183 - val_acc: 0.8476\n",
      "Epoch 131/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0057 - acc: 0.9874 - val_loss: 0.4158 - val_acc: 0.8476\n",
      "Epoch 132/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0056 - acc: 0.9886 - val_loss: 0.4157 - val_acc: 0.8490\n",
      "Epoch 133/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0053 - acc: 0.9893 - val_loss: 0.4174 - val_acc: 0.8476\n",
      "Epoch 134/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0056 - acc: 0.9885 - val_loss: 0.4143 - val_acc: 0.8453\n",
      "Epoch 135/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0055 - acc: 0.9879 - val_loss: 0.4161 - val_acc: 0.8448\n",
      "Epoch 136/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0054 - acc: 0.9889 - val_loss: 0.4204 - val_acc: 0.8453\n",
      "Epoch 137/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0054 - acc: 0.9886 - val_loss: 0.4192 - val_acc: 0.8472\n",
      "Epoch 138/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0054 - acc: 0.9890 - val_loss: 0.4198 - val_acc: 0.8453\n",
      "Epoch 139/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0054 - acc: 0.9884 - val_loss: 0.4194 - val_acc: 0.8481\n",
      "Epoch 140/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0053 - acc: 0.9885 - val_loss: 0.4192 - val_acc: 0.8462\n",
      "Epoch 141/200\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0054 - acc: 0.9882 - val_loss: 0.4240 - val_acc: 0.8476\n",
      "Epoch 142/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0053 - acc: 0.9888 - val_loss: 0.4244 - val_acc: 0.8467\n",
      "Epoch 143/200\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0055 - acc: 0.9890 - val_loss: 0.4227 - val_acc: 0.8490\n",
      "Epoch 144/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0053 - acc: 0.9882 - val_loss: 0.4221 - val_acc: 0.8439\n",
      "Epoch 145/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0054 - acc: 0.9877 - val_loss: 0.4253 - val_acc: 0.8481\n",
      "Epoch 146/200\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0053 - acc: 0.9884 - val_loss: 0.4238 - val_acc: 0.8467\n",
      "Epoch 147/200\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0052 - acc: 0.9883 - val_loss: 0.4266 - val_acc: 0.8472\n",
      "Epoch 148/200\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0053 - acc: 0.9883 - val_loss: 0.4227 - val_acc: 0.8518\n",
      "Epoch 149/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0053 - acc: 0.9884 - val_loss: 0.4207 - val_acc: 0.8490\n",
      "Epoch 150/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0052 - acc: 0.9886 - val_loss: 0.4196 - val_acc: 0.8495\n",
      "Epoch 151/200\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0051 - acc: 0.9881 - val_loss: 0.4237 - val_acc: 0.8467\n",
      "Epoch 152/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0052 - acc: 0.9888 - val_loss: 0.4255 - val_acc: 0.8472\n",
      "Epoch 153/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0052 - acc: 0.9884 - val_loss: 0.4236 - val_acc: 0.8495\n",
      "Epoch 154/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0052 - acc: 0.9882 - val_loss: 0.4258 - val_acc: 0.8476\n",
      "Epoch 155/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0052 - acc: 0.9884 - val_loss: 0.4267 - val_acc: 0.8444\n",
      "Epoch 156/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0052 - acc: 0.9887 - val_loss: 0.4267 - val_acc: 0.8490\n",
      "Epoch 157/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0052 - acc: 0.9881 - val_loss: 0.4276 - val_acc: 0.8467\n",
      "Epoch 158/200\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0052 - acc: 0.9888 - val_loss: 0.4318 - val_acc: 0.8472\n",
      "Epoch 159/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0051 - acc: 0.9887 - val_loss: 0.4297 - val_acc: 0.8472\n",
      "Epoch 160/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0051 - acc: 0.9891 - val_loss: 0.4295 - val_acc: 0.8486\n",
      "Epoch 161/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0052 - acc: 0.9887 - val_loss: 0.4323 - val_acc: 0.8462\n",
      "Epoch 162/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0051 - acc: 0.9886 - val_loss: 0.4299 - val_acc: 0.8462\n",
      "Epoch 163/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0052 - acc: 0.9886 - val_loss: 0.4344 - val_acc: 0.8462\n",
      "Epoch 164/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0051 - acc: 0.9882 - val_loss: 0.4319 - val_acc: 0.8481\n",
      "Epoch 165/200\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0051 - acc: 0.9889 - val_loss: 0.4295 - val_acc: 0.8495\n",
      "Epoch 166/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0052 - acc: 0.9883 - val_loss: 0.4348 - val_acc: 0.8462\n",
      "Epoch 167/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0051 - acc: 0.9882 - val_loss: 0.4349 - val_acc: 0.8500\n",
      "Epoch 168/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0051 - acc: 0.9884 - val_loss: 0.4326 - val_acc: 0.8490\n",
      "Epoch 169/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0051 - acc: 0.9881 - val_loss: 0.4331 - val_acc: 0.8481\n",
      "Epoch 170/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0052 - acc: 0.9882 - val_loss: 0.4319 - val_acc: 0.8481\n",
      "Epoch 171/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0050 - acc: 0.9881 - val_loss: 0.4355 - val_acc: 0.8490\n",
      "Epoch 172/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0051 - acc: 0.9887 - val_loss: 0.4340 - val_acc: 0.8481\n",
      "Epoch 173/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0051 - acc: 0.9888 - val_loss: 0.4322 - val_acc: 0.8458\n",
      "Epoch 174/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0053 - acc: 0.9882 - val_loss: 0.4315 - val_acc: 0.8490\n",
      "Epoch 175/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0052 - acc: 0.9890 - val_loss: 0.4290 - val_acc: 0.8481\n",
      "Epoch 176/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0051 - acc: 0.9887 - val_loss: 0.4346 - val_acc: 0.8486\n",
      "Epoch 177/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0050 - acc: 0.9882 - val_loss: 0.4367 - val_acc: 0.8462\n",
      "Epoch 178/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0051 - acc: 0.9886 - val_loss: 0.4353 - val_acc: 0.8490\n",
      "Epoch 179/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0052 - acc: 0.9882 - val_loss: 0.4367 - val_acc: 0.8500\n",
      "Epoch 180/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0050 - acc: 0.9882 - val_loss: 0.4367 - val_acc: 0.8481\n",
      "Epoch 181/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0050 - acc: 0.9894 - val_loss: 0.4360 - val_acc: 0.8490\n",
      "Epoch 182/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0051 - acc: 0.9882 - val_loss: 0.4383 - val_acc: 0.8495\n",
      "Epoch 183/200\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0050 - acc: 0.9886 - val_loss: 0.4352 - val_acc: 0.8481\n",
      "Epoch 184/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0050 - acc: 0.9882 - val_loss: 0.4366 - val_acc: 0.8467\n",
      "Epoch 185/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0051 - acc: 0.9885 - val_loss: 0.4373 - val_acc: 0.8495\n",
      "Epoch 186/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0052 - acc: 0.9884 - val_loss: 0.4365 - val_acc: 0.8500\n",
      "Epoch 187/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0050 - acc: 0.9894 - val_loss: 0.4354 - val_acc: 0.8444\n",
      "Epoch 188/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0050 - acc: 0.9892 - val_loss: 0.4400 - val_acc: 0.8481\n",
      "Epoch 189/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0050 - acc: 0.9888 - val_loss: 0.4361 - val_acc: 0.8500\n",
      "Epoch 190/200\n",
      "35/35 [==============================] - 2s 48ms/step - loss: 0.0050 - acc: 0.9887 - val_loss: 0.4368 - val_acc: 0.8472\n",
      "Epoch 191/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0049 - acc: 0.9882 - val_loss: 0.4342 - val_acc: 0.8472\n",
      "Epoch 192/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0049 - acc: 0.9879 - val_loss: 0.4372 - val_acc: 0.8518\n",
      "Epoch 193/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0051 - acc: 0.9884 - val_loss: 0.4394 - val_acc: 0.8448\n",
      "Epoch 194/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0050 - acc: 0.9883 - val_loss: 0.4388 - val_acc: 0.8458\n",
      "Epoch 195/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0053 - acc: 0.9888 - val_loss: 0.4416 - val_acc: 0.8439\n",
      "Epoch 196/200\n",
      "35/35 [==============================] - 2s 48ms/step - loss: 0.0050 - acc: 0.9879 - val_loss: 0.4427 - val_acc: 0.8458\n",
      "Epoch 197/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0052 - acc: 0.9886 - val_loss: 0.4366 - val_acc: 0.8430\n",
      "Epoch 198/200\n",
      "35/35 [==============================] - 2s 47ms/step - loss: 0.0066 - acc: 0.9874 - val_loss: 0.4319 - val_acc: 0.8495\n",
      "Epoch 199/200\n",
      "35/35 [==============================] - 2s 46ms/step - loss: 0.0051 - acc: 0.9884 - val_loss: 0.4295 - val_acc: 0.8490\n",
      "Epoch 200/200\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0050 - acc: 0.9886 - val_loss: 0.4324 - val_acc: 0.8462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0e6ec84518>"
      ]
     },
     "execution_count": 329,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9WwPwuq1w6V3"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/content/drive/My Drive/Colab Notebooks/LanguageTranslation/nmt_weights.h5') #saving model weight\n",
    "model.save('/content/drive/My Drive/Colab Notebooks/LanguageTranslation/model.h5') #saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaIYbSK1OiUH"
   },
   "outputs": [],
   "source": [
    "#inference setup\n",
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCxSGZb1OqQQ"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    #print('sequences',input_seq)\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    #print('states_values',states_value)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "        len(decoded_sentence) > 50):\n",
    "              stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GcrfvycyOtgN"
   },
   "outputs": [],
   "source": [
    "#THIS GENERATOR FUNCTIO CAN BE USE FOR DATA GENERATION IN CASE IF WE WANT TO USE FIT_GENERATOR INSTEAD OF FIT AND WE CAN USE TRAIN AND TEST DATA\n",
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3akObDTPeW4"
   },
   "outputs": [],
   "source": [
    "#RESULT TRAINING AND TESTING DATA AND GENERATE DATA USING GENERATOR_BATCH\n",
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "oyf_2-xKRy4o",
    "outputId": "29de6211-fd11-4b10-f8db-f612c2378f8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: he reminded his wife to wake him up at am\n",
      "Actual Hindi Translation:  उसने अपनी पत्नी को उसे सुबह सात बजे उठाने की बात का याद दिलाया। \n",
      "Predicted Hindi Translation:  उसने अपनी पत्नी को उसे सुबह सात बजे उठाने की बा\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "g0xfWbvxPjLv",
    "outputId": "af910f6a-496e-44af-e10b-cdadf706f2f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: im looking for a small suitcase\n",
      "Actual Hindi Translation:  मैं एक छोटी अटैची को ढूँढ रहा हूँ। \n",
      "Predicted Hindi Translation:  मैं एक छोटी अटैची को ढूँढ रहा हूँ। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "-NW5ktuYUnTX",
    "outputId": "eda50fef-48d4-4104-ac74-9273b5f96b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: he usually comes home late\n",
      "Actual Hindi Translation:  वह आमतौर पर घर देर से आता है। \n",
      "Predicted Hindi Translation:  वह आमतौर पर घर देर से आता है। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "qHUaY9L_UqlU",
    "outputId": "8e52784f-bab9-4243-84b5-524849a009c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: i cant walk any further\n",
      "Actual Hindi Translation:  मैं और नहीं चल सकती। \n",
      "Predicted Hindi Translation:  मुझसे और नहीं चला जाएगा। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Vyz8EOE1__7s",
    "outputId": "be34b444-ab1e-47bd-e9a1-9b4778ad2e06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: you shouldve come yesterday\n",
      "Actual Hindi Translation:  तुम्हे कल आना चाहिए था। \n",
      "Predicted Hindi Translation:  तुम्हे कल आना चाहिए था। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "WzC31ew0CXML",
    "outputId": "17b760fc-430e-4fc9-d43e-00fb622a87ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: my hobby is visiting old temples\n",
      "Actual Hindi Translation:  मैं पुराने मंदिरों को देखने जाने का शौकीन हूँ। \n",
      "Predicted Hindi Translation:  मैं पुराने मंदिरों को देखने जाने का शौकीन हूँ। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVIqB0jbCZx1"
   },
   "outputs": [],
   "source": [
    "#TESTING ON TESTING DATA\n",
    "train_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
    "k=-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "wfgrtjBhMncy",
    "outputId": "7a01d5c5-60d0-4d6b-ecdd-d0f6afc70fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: dont shout\n",
      "Actual Hindi Translation:  चिल्लाओ मत \n",
      "Predicted Hindi Translation:  चिल्लाईए मत। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "v7kJ_k-mMpLZ",
    "outputId": "5e41b370-1965-4253-c814-297261f05566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: we ordered pink but we received blue\n",
      "Actual Hindi Translation:  हमने पिंक रंग वाले का ऑर्डर किया था पर हमे नीले रंग वाला मिला। \n",
      "Predicted Hindi Translation:  हमने मामले की देखभाल अच्छी खुशी से गए। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Q4YtcZLpMroQ",
    "outputId": "be8cac42-9ed6-4b76-8190-b159424bac00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: no answer is also an answer\n",
      "Actual Hindi Translation:  जवाब न देना भी एक तरह का जवाब होता है। \n",
      "Predicted Hindi Translation:  तुम्हारी उम्र दिन मुझे लिखा है वह सच है। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "K8-IsGyaMt4q",
    "outputId": "2c463ec3-d89b-44fa-c301-31eb7e1b5d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: this watch is broken\n",
      "Actual Hindi Translation:  यह घड़ी टूटी हुई है। \n",
      "Predicted Hindi Translation:  इस दवाई को है। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "5ehy1pK8MvtV",
    "outputId": "3b361e10-166b-437d-e1ae-dc082a71c3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: i want you to go to osaka at once\n",
      "Actual Hindi Translation:  फ़ौरन ओसाका जाओ। \n",
      "Predicted Hindi Translation:  मैं एक बूढ़े आदमी की तलाश की सलाह वाला हूँ। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Hhwb1LjGMwrS",
    "outputId": "c618f916-b188-4fe7-ec56-498340d8472c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: i like the blue one how much does it cost\n",
      "Actual Hindi Translation:  मुझे नीली वाली पसंद है कितने की है \n",
      "Predicted Hindi Translation:  मुझे नीला वाला पसंद है कितने का है \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "J6wA2C8BMxhW",
    "outputId": "1f72385d-41e5-4d75-d15c-4c4d85c29628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: i am grateful to them\n",
      "Actual Hindi Translation:  मैं उनका आभारी हूँ। \n",
      "Predicted Hindi Translation:  मैं परेशान था कि तो। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Utac6fu0MzEC",
    "outputId": "1cc6e02d-6206-47b9-dea5-c51e4be53c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: the road is parallel to the river\n",
      "Actual Hindi Translation:  वह सड़क नदी के साथ साथ चलती है। \n",
      "Predicted Hindi Translation:  एक टोपी इनसान की संगीत के पर बहुत खुश करो। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "EXExr5CeM0J3",
    "outputId": "277132a1-4da6-47bc-ac0c-6b33447873a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: i still dont know\n",
      "Actual Hindi Translation:  मुझे अभी भी नहीं पता। \n",
      "Predicted Hindi Translation:  मुझे नहीं पता। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Z5q8txHiM1Kv",
    "outputId": "2631bdb0-227d-4ead-cc11-9ae8afcaf835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: were here to protect you\n",
      "Actual Hindi Translation:  हम आपकी रक्षा करने के लिए यहाँ आए हैं। \n",
      "Predicted Hindi Translation:  आपकी रक्षा करना हमारा काम है। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "mwykvnvaM2NF",
    "outputId": "0112a413-5051-486b-a961-be9fe6c0272f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: he has a lot of money\n",
      "Actual Hindi Translation:  वह बहुत पैसेवाला है। \n",
      "Predicted Hindi Translation:  उसके अभी बहुत शौक लगती है। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "tmNZIWz5M3JC",
    "outputId": "2cc1ab6d-8d25-4d47-8683-3fc429b695d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: no i didnt go\n",
      "Actual Hindi Translation:  नहीं मैं नहीं गया था। \n",
      "Predicted Hindi Translation:  मुझे नहीं पता गया कि क्या \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "ozOjOgB8M4Rp",
    "outputId": "98182dee-51ba-4223-d2c5-2332be9b8acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: we must pay the tax\n",
      "Actual Hindi Translation:  हमें टैक्स भरना पड़ेगा। \n",
      "Predicted Hindi Translation:  हमे अच्छी तक पहुँच गए। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Y3cUhDkIM5pe",
    "outputId": "06a6e180-97ed-40a3-a191-b574ba175502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: the trees are green\n",
      "Actual Hindi Translation:  पेड़ हरे हैं। \n",
      "Predicted Hindi Translation:  कृपया पेड़ हैं। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "C7ZgDVcXM6ty",
    "outputId": "dcbe97a9-4aac-419f-eff8-fbbefd13db4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: youre kidding\n",
      "Actual Hindi Translation:  मज़ाक कर रहे हो \n",
      "Predicted Hindi Translation:  मौज करना। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "E0SAfTyNM7oD",
    "outputId": "88d9cf01-6493-42c5-dcce-047c71fc013f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: wash your hands before meals\n",
      "Actual Hindi Translation:  खाना खाने से पहले अपने हाथ धोओ। \n",
      "Predicted Hindi Translation:  मेरी हाथ चल हैं। \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QoLmqR_CPDPQ"
   },
   "source": [
    "# TESTING ON USER INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ti9Mfj7SPPvn"
   },
   "outputs": [],
   "source": [
    "def translation():\n",
    "    \n",
    "    list_exit_word=['exit','break','leave','bye']\n",
    "    \n",
    "    while True:\n",
    "        input_text=input('Enter the text you want to translate \\t')\n",
    "        if input_text not in list_exit_word:\n",
    "            thought_matrix=convert_user_input_into_mat(input_text)\n",
    "            response=decode_sequence(thought_matrix)\n",
    "            response=response.replace('_END','') #replacing _END with space\n",
    "            print('original text \\t',input_text)\n",
    "            print('translated text \\t',response)\n",
    "        else:\n",
    "            print('Have good day')\n",
    "            return \n",
    "\n",
    "\n",
    "def convert_user_input_into_mat(input_text): #this method is same as we did above with data before training\n",
    "    input_text=input_text.lower()\n",
    "    encoder_input_data = np.zeros((1, max_length_src),dtype='float32') #creating matrix of max_le\n",
    "\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "      #print(word)\n",
    "      encoder_input_data[0, t] = input_token_index[word] #input_token_index is dict and our vocabulary is limited to if we enter any word which is not present in our vocab we \n",
    "      #will get an error\n",
    "\n",
    "    return encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "Mf2I07q0TW0q",
    "outputId": "644c4a1b-6396-45e5-f859-8590b3420d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the text you want to translate \tHOW ARE YOU\n",
      "original text \t HOW ARE YOU\n",
      "translated text \t  तुम कैसी हो \n",
      "Enter the text you want to translate \tI AM GOOD\n",
      "original text \t I AM GOOD\n",
      "translated text \t  मुझे अंग्रेज़ी अच्छा लगता है। \n",
      "Enter the text you want to translate \tWHAT IS YOUR AGE\n",
      "original text \t WHAT IS YOUR AGE\n",
      "translated text \t  तुम्हारी उम्र क्या है \n",
      "Enter the text you want to translate \ti do not know\n",
      "original text \t i do not know\n",
      "translated text \t  मुझे नहीं पता। \n",
      "Enter the text you want to translate \tok\n",
      "original text \t ok\n",
      "translated text \t  मैं समझता हूँ। \n",
      "Enter the text you want to translate \ttake care\n",
      "original text \t take care\n",
      "translated text \t  बचाओ \n",
      "Enter the text you want to translate \thello\n",
      "original text \t hello\n",
      "translated text \t  नमस्ते। \n",
      "Enter the text you want to translate \tgood\n",
      "original text \t good\n",
      "translated text \t  मज़े करना। \n",
      "Enter the text you want to translate \texit\n",
      "Have good day\n"
     ]
    }
   ],
   "source": [
    "translation()#AS WE CAN SEE THAT THE ACCURACY IS NOT SO GOOD BECAUSE OF 2 MAJOR REASONS FIRST ONE IS THE DATA IS VERY SMALL AND SECOND ONE IS BECAUSE OF SMALL DATASET \n",
    "#UNCERTAINITY IS HIGH IN THE PERFORMANCE WE CAN USE THE CHARACTER LEVEL TRANSLATION INSTEAD OF WORD LEVEL BUT IN MOST OF CASES WORD LEVEL TRASNLATION IS USED\n",
    "#AND ONE ADVANTAGE OF CHARACTER LEVEL TRANSLATION OVER WORD LEVEL IS THAT IN WORD LEVEL WE HAVE LIMITED VOCAB SO IF ANY WORD IS NOT PRESENT IN VOCAB THEN WE WILL GET AN ERROR\n",
    "#BUT IN CASE OF CHARACTERS AS WE KNOW CHARACTERS ARE LIMITED IN EVERY LANGUAGE WE WILL GET GOOD RESULT COMPARATIVE TO WORD LEVEL SPEACIALLY WHEN WE HAVE SMALL DATASET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97j65F_uUH2K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "English_to_Hindi_Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
